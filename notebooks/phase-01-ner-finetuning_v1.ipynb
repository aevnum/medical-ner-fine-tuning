{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75557327",
   "metadata": {},
   "source": [
    "# 0. Project Configuration & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782e577",
   "metadata": {},
   "source": [
    "## 0.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 0.1 IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine Learning & NLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformers & Datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from seqeval.metrics import (\n",
    "    classification_report as seqeval_classification_report,\n",
    "    f1_score as seqeval_f1_score,\n",
    "    precision_score as seqeval_precision_score,\n",
    "    recall_score as seqeval_recall_score\n",
    ")\n",
    "\n",
    "# Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "print(\"\\n✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e75a6c",
   "metadata": {},
   "source": [
    "## 0.2 Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 0.2 CONFIGURATION CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "# Disable parallelism for tokenizers to avoid warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Project Information\n",
    "PROJECT_NAME = \"Medical_Entity_Recognition_Linking\"\n",
    "\n",
    "# Timestamp for experiment tracking\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXPERIMENT_NAME = f\"MERL_{TIMESTAMP}\"\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_NAME = \"ktgiahieu/maccrobat2018_2020\"\n",
    "DATASET_SPLIT_SIZES = {\n",
    "    'train': 340,\n",
    "    'validation': 30,\n",
    "    'test': 30\n",
    "}\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_CONFIGS = {\n",
    "    'pubmedbert': {\n",
    "        'name': 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext',\n",
    "        'display_name': 'PubMedBERT'\n",
    "    },\n",
    "    'biobert': {\n",
    "        'name': 'dmis-lab/biobert-v1.1',\n",
    "        'display_name': 'BioBERT'\n",
    "    },\n",
    "    'bioformer': {\n",
    "        'name': 'bioformers/bioformer-16L',\n",
    "        'display_name': 'Bioformer-16L'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Primary model for baseline\n",
    "PRIMARY_MODEL = 'pubmedbert'\n",
    "PRIMARY_MODEL_NAME = MODEL_CONFIGS[PRIMARY_MODEL]['name']\n",
    "\n",
    "# Training Hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 5,\n",
    "    'warmup_steps': 500,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'fp16': torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    'logging_steps': 50,\n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 100,\n",
    "    'save_total_limit': 2,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'f1',\n",
    "    'greater_is_better': True\n",
    "}\n",
    "\n",
    "# Entity Type Tiers (from EDA)\n",
    "ENTITY_TIERS = {\n",
    "    'frequent': [\n",
    "        'Age', 'Diagnostic_procedure', 'Sign_symptom', 'Lab_value',\n",
    "        'Biological_structure', 'Detailed_description', 'Date',\n",
    "        'Disease_disorder', 'History', 'Therapeutic_procedure',\n",
    "        'Medication', 'Dosage', 'Duration', 'Clinical_event',\n",
    "        'Nonbiological_location'\n",
    "    ],\n",
    "    'medium': [\n",
    "        'Family_history', 'Coreference', 'Sex', 'Distance', 'Other_entity',\n",
    "        'Area', 'Volume', 'Time', 'Frequency', 'Activity', 'Other_event',\n",
    "        'Personal_background', 'Administration'\n",
    "    ],\n",
    "    'rare': [\n",
    "        'Subject', 'Occupation', 'Outcome', 'Shape', 'Severity',\n",
    "        'Qualitative_concept', 'Quantitative_concept', 'Texture',\n",
    "        'Color', 'Height', 'Weight', 'Biological_attribute', 'Mass'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# High-value entities for medical coding (production focus)\n",
    "HIGH_VALUE_ENTITIES = [\n",
    "    'Disease_disorder',\n",
    "    'Therapeutic_procedure',\n",
    "    'Medication',\n",
    "    'Sign_symptom',\n",
    "    'Diagnostic_procedure'\n",
    "]\n",
    "\n",
    "# Confidence Thresholds to Test\n",
    "CONFIDENCE_THRESHOLDS = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "# Output Directories\n",
    "OUTPUT_DIR = f\"./outputs/{EXPERIMENT_NAME}\"\n",
    "MODEL_SAVE_DIR = f\"{OUTPUT_DIR}/models\"\n",
    "RESULTS_DIR = f\"{OUTPUT_DIR}/results\"\n",
    "PLOTS_DIR = f\"{OUTPUT_DIR}/plots\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Random Seeds for Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seed(seed: int = RANDOM_SEED):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# Display Configuration\n",
    "print(\"=\"*80)\n",
    "print(f\"PROJECT: {PROJECT_NAME}\")\n",
    "print(f\"EXPERIMENT: {EXPERIMENT_NAME}\")\n",
    "print(f\"TIMESTAMP: {TIMESTAMP}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrimary Model: {MODEL_CONFIGS[PRIMARY_MODEL]['display_name']}\")\n",
    "print(f\"Model Path: {PRIMARY_MODEL_NAME}\")\n",
    "print(f\"\\nDataset: {DATASET_NAME}\")\n",
    "print(f\"Train: {DATASET_SPLIT_SIZES['train']} | \"\n",
    "      f\"Val: {DATASET_SPLIT_SIZES['validation']} | \"\n",
    "      f\"Test: {DATASET_SPLIT_SIZES['test']}\")\n",
    "print(f\"\\nTraining Config:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nOutput Directory: {OUTPUT_DIR}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954d963",
   "metadata": {},
   "source": [
    "## 0.3 WandB Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28003171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 0.3 WANDB INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Method 1: Using Kaggle Secrets (if available)\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    os.environ['WANDB_API_KEY'] = wandb_api_key\n",
    "    print(\"✓ WandB API key loaded from Kaggle Secrets\")\n",
    "except:\n",
    "    print(\"⚠ Kaggle Secrets not available. WandB will prompt for login.\")\n",
    "    print(\"  You can manually set: os.environ['WANDB_API_KEY'] = 'your_key_here'\")\n",
    "\n",
    "# Initialize WandB\n",
    "try:\n",
    "    # Login to WandB\n",
    "    wandb.login()\n",
    "    \n",
    "    # Initialize WandB project\n",
    "    wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=EXPERIMENT_NAME,\n",
    "        config={\n",
    "            \"model_name\": PRIMARY_MODEL_NAME,\n",
    "            \"dataset\": DATASET_NAME,\n",
    "            \"timestamp\": TIMESTAMP,\n",
    "            **TRAINING_CONFIG,\n",
    "            \"entity_tiers\": ENTITY_TIERS,\n",
    "            \"high_value_entities\": HIGH_VALUE_ENTITIES,\n",
    "            \"random_seed\": RANDOM_SEED\n",
    "        },\n",
    "        tags=[PRIMARY_MODEL],\n",
    "        notes=f\"Baseline NER model training on MACCROBAT dataset. \"\n",
    "              f\"Focus: production-readiness metrics and error analysis.\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"✓ WandB initialized successfully!\")\n",
    "    print(f\"  Project: {PROJECT_NAME}\")\n",
    "    print(f\"  Run Name: {EXPERIMENT_NAME}\")\n",
    "    print(f\"  Dashboard: {wandb.run.get_url()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    WANDB_ENABLED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ WandB initialization failed: {e}\")\n",
    "    print(\"  Continuing without WandB logging...\")\n",
    "    WANDB_ENABLED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b3485",
   "metadata": {},
   "source": [
    "## 0.4 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71db34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 0.4 HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def log_to_wandb(data: Dict, step: Optional[int] = None, commit: bool = True):\n",
    "    \"\"\"\n",
    "    Safely log data to WandB if enabled\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary of metrics to log\n",
    "        step: Optional step number\n",
    "        commit: Whether to commit the log immediately\n",
    "    \"\"\"\n",
    "    if WANDB_ENABLED:\n",
    "        try:\n",
    "            wandb.log(data, step=step, commit=commit)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to log to WandB: {e}\")\n",
    "\n",
    "\n",
    "def save_results(data: Dict, filename: str, directory: str = RESULTS_DIR):\n",
    "    \"\"\"\n",
    "    Save results to JSON file\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary to save\n",
    "        filename: Name of the file (with .json extension)\n",
    "        directory: Directory to save to\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"✓ Results saved to: {filepath}\")\n",
    "    \n",
    "    # Also log to WandB as artifact\n",
    "    if WANDB_ENABLED:\n",
    "        try:\n",
    "            artifact = wandb.Artifact(\n",
    "                name=filename.replace('.json', ''),\n",
    "                type='results'\n",
    "            )\n",
    "            artifact.add_file(filepath)\n",
    "            wandb.log_artifact(artifact)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to log artifact to WandB: {e}\")\n",
    "\n",
    "\n",
    "def save_plot(fig, filename: str, directory: str = PLOTS_DIR, dpi: int = 300):\n",
    "    \"\"\"\n",
    "    Save matplotlib figure and log to WandB\n",
    "    \n",
    "    Args:\n",
    "        fig: Matplotlib figure object\n",
    "        filename: Name of the file (with extension)\n",
    "        directory: Directory to save to\n",
    "        dpi: Resolution for saved figure\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
    "    print(f\"✓ Plot saved to: {filepath}\")\n",
    "    \n",
    "    # Log to WandB\n",
    "    if WANDB_ENABLED:\n",
    "        try:\n",
    "            wandb.log({filename.replace('.png', ''): wandb.Image(filepath)})\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to log plot to WandB: {e}\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Format seconds into human-readable time string\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.2f}m\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.2f}h\"\n",
    "\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB | Reserved: {reserved:.2f}GB\")\n",
    "\n",
    "\n",
    "def create_summary_table(results_dict: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a formatted summary table from results dictionary\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary with model names as keys and metrics as values\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame with formatted results\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results_dict).T\n",
    "    df = df.round(4)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Helper functions defined successfully!\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - log_to_wandb(): Log metrics to WandB\")\n",
    "print(\"  - save_results(): Save results to JSON\")\n",
    "print(\"  - save_plot(): Save and log matplotlib figures\")\n",
    "print(\"  - format_time(): Convert seconds to readable format\")\n",
    "print(\"  - print_gpu_memory(): Check GPU usage\")\n",
    "print(\"  - create_summary_table(): Format results as DataFrame\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a4f94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948291bd",
   "metadata": {},
   "source": [
    "# 1. Environment Setup & Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94207b02",
   "metadata": {},
   "source": [
    "## 1.1 Load MACCROBAT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.1 LOAD MACCROBAT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Loading MACCROBAT Dataset...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the dataset from HuggingFace\n",
    "medical_ner_data = load_dataset(DATASET_NAME)\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(medical_ner_data)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "for split in medical_ner_data.keys():\n",
    "    print(f\"  {split}: {len(medical_ner_data[split])} examples\")\n",
    "\n",
    "# Examine first example\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Example:\")\n",
    "print(\"=\"*80)\n",
    "example = medical_ner_data['train'][0]\n",
    "print(f\"Keys: {list(example.keys())}\")\n",
    "print(f\"\\nFirst 10 tokens: {example['tokens'][:10]}\")\n",
    "print(f\"First 10 tags: {example['tags'][:10]}\")\n",
    "print(f\"\\nTotal tokens in example: {len(example['tokens'])}\")\n",
    "print(f\"Total tags in example: {len(example['tags'])}\")\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'dataset/total_examples': len(medical_ner_data['train']),\n",
    "    'dataset/name': DATASET_NAME\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b0249",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.2 DATASET STRUCTURE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Dataset Structure Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all unique tags\n",
    "all_tags = set()\n",
    "for example in medical_ner_data['train']:\n",
    "    all_tags.update(example['tags'])\n",
    "\n",
    "print(f\"\\nUnique tags found: {len(all_tags)}\")\n",
    "print(f\"\\nSample tags:\")\n",
    "for tag in sorted(list(all_tags))[:20]:\n",
    "    print(f\"  - {tag}\")\n",
    "\n",
    "# Count tag frequencies\n",
    "tag_counter = Counter()\n",
    "for example in medical_ner_data['train']:\n",
    "    tag_counter.update(example['tags'])\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Top 10 Most Frequent Tags:\")\n",
    "print(\"=\"*80)\n",
    "for tag, count in tag_counter.most_common(10):\n",
    "    print(f\"  {tag:<30} : {count:>6} occurrences\")\n",
    "\n",
    "# Document length statistics\n",
    "doc_lengths = [len(example['tokens']) for example in medical_ner_data['train']]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Document Length Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Mean length: {np.mean(doc_lengths):.2f} tokens\")\n",
    "print(f\"  Median length: {np.median(doc_lengths):.2f} tokens\")\n",
    "print(f\"  Std deviation: {np.std(doc_lengths):.2f}\")\n",
    "print(f\"  Min length: {np.min(doc_lengths)} tokens\")\n",
    "print(f\"  Max length: {np.max(doc_lengths)} tokens\")\n",
    "\n",
    "# Entity count per document\n",
    "entity_counts = []\n",
    "for example in medical_ner_data['train']:\n",
    "    # Count B- tags (each represents one entity)\n",
    "    entity_count = sum(1 for tag in example['tags'] if tag.startswith('B-'))\n",
    "    entity_counts.append(entity_count)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Entities per Document:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Mean entities: {np.mean(entity_counts):.2f}\")\n",
    "print(f\"  Median entities: {np.median(entity_counts):.2f}\")\n",
    "print(f\"  Min entities: {np.min(entity_counts)}\")\n",
    "print(f\"  Max entities: {np.max(entity_counts)}\")\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'dataset/unique_tags': len(all_tags),\n",
    "    'dataset/mean_doc_length': np.mean(doc_lengths),\n",
    "    'dataset/mean_entities_per_doc': np.mean(entity_counts)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e4b17",
   "metadata": {},
   "source": [
    "## 1.3 Tag Mapping Creation (tag2id, id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.3 TAG MAPPING CREATION (tag2id, id2tag)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Creating Tag Mappings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all unique tags from dataset\n",
    "unique_tags = set()\n",
    "for example in medical_ner_data['train']:\n",
    "    unique_tags.update(example['tags'])\n",
    "\n",
    "# Ensure every B-XXX tag has a corresponding I-XXX tag\n",
    "# This is important for proper BIO tagging\n",
    "extra_i_tags = set()\n",
    "for tag in unique_tags:\n",
    "    if tag.startswith(\"B-\"):\n",
    "        entity_type = tag[2:]  # Remove 'B-' prefix\n",
    "        i_tag = f\"I-{entity_type}\"\n",
    "        if i_tag not in unique_tags:\n",
    "            extra_i_tags.add(i_tag)\n",
    "            print(f\"  Adding missing I-tag: {i_tag}\")\n",
    "\n",
    "unique_tags.update(extra_i_tags)\n",
    "\n",
    "# Sort tags for consistent ordering\n",
    "unique_tags = sorted(list(unique_tags))\n",
    "\n",
    "print(f\"\\nTotal unique tags (including added I-tags): {len(unique_tags)}\")\n",
    "\n",
    "# Create mappings\n",
    "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
    "\n",
    "# Display sample mappings\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Tag Mappings:\")\n",
    "print(\"=\"*80)\n",
    "for i, (tag, idx) in enumerate(list(tag2id.items())[:15]):\n",
    "    print(f\"  {tag:<30} -> {idx:>3}\")\n",
    "\n",
    "print(f\"\\n...\" )\n",
    "print(f\"  (Total: {len(tag2id)} tags)\")\n",
    "\n",
    "# Save mappings\n",
    "mappings = {\n",
    "    'tag2id': tag2id,\n",
    "    'id2tag': id2tag,\n",
    "    'num_labels': len(unique_tags)\n",
    "}\n",
    "\n",
    "save_results(mappings, 'tag_mappings.json')\n",
    "\n",
    "# Store in global variables for later use\n",
    "NUM_LABELS = len(unique_tags)\n",
    "\n",
    "print(f\"\\n✓ Tag mappings created successfully!\")\n",
    "print(f\"  Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'model/num_labels': NUM_LABELS\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12be8c2",
   "metadata": {},
   "source": [
    "## 1.4 Entity Type Tier Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.4 ENTITY TYPE TIER CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Entity Type Tier Classification\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CONFIGURATION: Toggle to remove rare entity types\n",
    "REMOVE_RARE_ENTITIES = False  # Set to True to convert rare entities to 'O'\n",
    "RARE_THRESHOLD = 100  # Entities with count < threshold are considered rare\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Remove rare entities: {REMOVE_RARE_ENTITIES}\")\n",
    "print(f\"  Rare threshold: < {RARE_THRESHOLD} occurrences\")\n",
    "\n",
    "# Extract entity types from tags (B-XXX and I-XXX)\n",
    "entity_type_counts = Counter()\n",
    "\n",
    "for example in medical_ner_data['train']:\n",
    "    for tag in example['tags']:\n",
    "        if tag != 'O' and '-' in tag:\n",
    "            entity_type = tag.split('-')[1]\n",
    "            entity_type_counts[entity_type] += 1\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "entity_df = pd.DataFrame(\n",
    "    entity_type_counts.items(),\n",
    "    columns=['Entity_Type', 'Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "# Classify into tiers based on frequency\n",
    "def classify_tier(count):\n",
    "    if count > 1000:\n",
    "        return 'frequent'\n",
    "    elif count >= RARE_THRESHOLD:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'rare'\n",
    "\n",
    "entity_df['Tier'] = entity_df['Count'].apply(classify_tier)\n",
    "\n",
    "print(f\"\\nEntity Type Distribution:\")\n",
    "print(entity_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Tier Summary:\")\n",
    "print(\"=\"*80)\n",
    "tier_summary = entity_df['Tier'].value_counts()\n",
    "for tier, count in tier_summary.items():\n",
    "    print(f\"  {tier.capitalize():<10} : {count:>2} entity types\")\n",
    "\n",
    "# Identify rare entity types\n",
    "rare_entity_types = entity_df[entity_df['Tier'] == 'rare']['Entity_Type'].tolist()\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Rare Entity Types ({len(rare_entity_types)} total):\")\n",
    "print(\"=\"*80)\n",
    "for entity_type in rare_entity_types:\n",
    "    count = entity_df[entity_df['Entity_Type'] == entity_type]['Count'].values[0]\n",
    "    print(f\"  - {entity_type:<30} : {count:>3} occurrences\")\n",
    "\n",
    "# Function to remove rare entities from dataset\n",
    "def remove_rare_entities_from_dataset(dataset, rare_entities):\n",
    "    \"\"\"\n",
    "    Convert rare entity tags to 'O' in the dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset\n",
    "        rare_entities: List of entity types to remove\n",
    "    \n",
    "    Returns:\n",
    "        Modified dataset with rare entities converted to 'O'\n",
    "    \"\"\"\n",
    "    def convert_tags(example):\n",
    "        new_tags = []\n",
    "        for tag in example['tags']:\n",
    "            if tag != 'O' and '-' in tag:\n",
    "                entity_type = tag.split('-')[1]\n",
    "                if entity_type in rare_entities:\n",
    "                    # Convert rare entity to 'O'\n",
    "                    new_tags.append('O')\n",
    "                else:\n",
    "                    new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag)\n",
    "        \n",
    "        example['tags'] = new_tags\n",
    "        return example\n",
    "    \n",
    "    # Apply conversion\n",
    "    modified_dataset = dataset.map(convert_tags, desc=\"Removing rare entities\")\n",
    "    \n",
    "    return modified_dataset\n",
    "\n",
    "\n",
    "# Apply rare entity removal if enabled\n",
    "if REMOVE_RARE_ENTITIES:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"REMOVING RARE ENTITIES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Converting {len(rare_entity_types)} rare entity types to 'O'...\")\n",
    "    \n",
    "    # Store original dataset for reference\n",
    "    medical_ner_data_original = medical_ner_data\n",
    "    \n",
    "    # Remove rare entities from train split\n",
    "    medical_ner_data['train'] = remove_rare_entities_from_dataset(\n",
    "        medical_ner_data['train'],\n",
    "        rare_entity_types\n",
    "    )\n",
    "    \n",
    "    # Count entities before and after\n",
    "    original_entity_count = sum(\n",
    "        1 for ex in medical_ner_data_original['train'] \n",
    "        for tag in ex['tags'] if tag.startswith('B-')\n",
    "    )\n",
    "    new_entity_count = sum(\n",
    "        1 for ex in medical_ner_data['train'] \n",
    "        for tag in ex['tags'] if tag.startswith('B-')\n",
    "    )\n",
    "    \n",
    "    removed_count = original_entity_count - new_entity_count\n",
    "    \n",
    "    print(f\"\\n✓ Rare entities removed!\")\n",
    "    print(f\"  Original entities: {original_entity_count}\")\n",
    "    print(f\"  Remaining entities: {new_entity_count}\")\n",
    "    print(f\"  Removed: {removed_count} ({removed_count/original_entity_count*100:.2f}%)\")\n",
    "    \n",
    "    # Update entity_df to reflect removal\n",
    "    entity_df = entity_df[entity_df['Tier'] != 'rare'].copy()\n",
    "    \n",
    "    print(f\"\\n  Remaining entity types: {len(entity_df)}\")\n",
    "    print(f\"    Frequent: {len(entity_df[entity_df['Tier'] == 'frequent'])}\")\n",
    "    print(f\"    Medium: {len(entity_df[entity_df['Tier'] == 'medium'])}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n✓ Keeping all entity types (including rare)\")\n",
    "\n",
    "# Verify predefined tiers match data\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Validating Predefined Tiers:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tier_name, tier_entities in ENTITY_TIERS.items():\n",
    "    # Skip rare tier validation if rare entities were removed\n",
    "    if REMOVE_RARE_ENTITIES and tier_name == 'rare':\n",
    "        print(f\"\\nRare Tier: [SKIPPED - rare entities removed]\")\n",
    "        continue\n",
    "    \n",
    "    actual_tier_entities = entity_df[entity_df['Tier'] == tier_name]['Entity_Type'].tolist()\n",
    "    \n",
    "    # Check if predefined entities are in data\n",
    "    missing = set(tier_entities) - set(actual_tier_entities)\n",
    "    extra = set(actual_tier_entities) - set(tier_entities)\n",
    "    \n",
    "    print(f\"\\n{tier_name.capitalize()} Tier:\")\n",
    "    print(f\"  Predefined: {len(tier_entities)} entity types\")\n",
    "    print(f\"  In dataset: {len(actual_tier_entities)} entity types\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"  ⚠ Missing from data: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"  ⚠ Extra in data: {extra}\")\n",
    "    if not missing and not extra:\n",
    "        print(f\"  ✓ Perfect match!\")\n",
    "\n",
    "# Save entity distribution\n",
    "save_results(\n",
    "    entity_df.to_dict('records'),\n",
    "    'entity_type_distribution.json'\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of entity frequencies (log scale)\n",
    "colors = entity_df['Tier'].map({'frequent': 'steelblue', 'medium': 'coral', 'rare': 'lightgreen'})\n",
    "ax1.barh(entity_df['Entity_Type'], entity_df['Count'], color=colors)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Count (log scale)')\n",
    "ax1.set_ylabel('Entity Type')\n",
    "ax1.set_title(f\"Entity Type Frequency Distribution{' (Rare Removed)' if REMOVE_RARE_ENTITIES else ''}\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pie chart of tier distribution\n",
    "tier_counts = entity_df.groupby('Tier')['Count'].sum()\n",
    "ax2.pie(\n",
    "    tier_counts.values,\n",
    "    labels=[f\"{t.capitalize()}\\n({c:,} instances)\" for t, c in zip(tier_counts.index, tier_counts.values)],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['steelblue', 'coral', 'lightgreen']\n",
    ")\n",
    "ax2.set_title(f\"Entity Distribution by Tier{' (Rare Removed)' if REMOVE_RARE_ENTITIES else ''}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_plot(fig, 'entity_tier_distribution.png')\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'dataset/entity_types_total': len(entity_df),\n",
    "    'dataset/frequent_entities': len(entity_df[entity_df['Tier'] == 'frequent']),\n",
    "    'dataset/medium_entities': len(entity_df[entity_df['Tier'] == 'medium']),\n",
    "    'dataset/rare_entities': len(entity_df[entity_df['Tier'] == 'rare']),\n",
    "    'config/remove_rare_entities': REMOVE_RARE_ENTITIES,\n",
    "    'config/rare_threshold': RARE_THRESHOLD\n",
    "})\n",
    "\n",
    "print(f\"\\n✓ Entity tier classification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04bc9c",
   "metadata": {},
   "source": [
    "## 1.5 Dataset Splitting (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.5 DATASET SPLITTING (Train/Val/Test)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Dataset Splitting\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the predefined split sizes\n",
    "train_size = DATASET_SPLIT_SIZES['train']\n",
    "val_size = DATASET_SPLIT_SIZES['validation']\n",
    "test_size = DATASET_SPLIT_SIZES['test']\n",
    "\n",
    "# Get the full dataset\n",
    "full_dataset = medical_ner_data['train']\n",
    "total_size = len(full_dataset)\n",
    "\n",
    "print(f\"\\nTotal dataset size: {total_size}\")\n",
    "print(f\"Target split: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "\n",
    "# Shuffle and split\n",
    "indices = list(range(total_size))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:train_size + val_size + test_size]\n",
    "\n",
    "# Create split datasets\n",
    "train_dataset = full_dataset.select(train_indices)\n",
    "val_dataset = full_dataset.select(val_indices)\n",
    "test_dataset = full_dataset.select(test_indices)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_split = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Split Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Train: {len(train_dataset)} examples ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_dataset)} examples ({len(test_dataset)/total_size*100:.1f}%)\")\n",
    "\n",
    "# Verify entity distribution across splits\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Entity Distribution Across Splits:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split_name, split_data in [('Train', train_dataset), ('Val', val_dataset), ('Test', test_dataset)]:\n",
    "    entity_count = 0\n",
    "    for example in split_data:\n",
    "        entity_count += sum(1 for tag in example['tags'] if tag.startswith('B-'))\n",
    "    \n",
    "    avg_entities = entity_count / len(split_data)\n",
    "    print(f\"  {split_name:<12} : {entity_count:>5} entities, {avg_entities:.2f} per doc\")\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'dataset/train_size': len(train_dataset),\n",
    "    'dataset/val_size': len(val_dataset),\n",
    "    'dataset/test_size': len(test_dataset)\n",
    "})\n",
    "\n",
    "print(f\"\\n✓ Dataset split complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77cb28",
   "metadata": {},
   "source": [
    "## 1.6 Tokenization & Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.6 TOKENIZATION & DATA COLLATOR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Setting up Tokenization & Data Collator\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load tokenizer for primary model\n",
    "print(f\"\\nLoading tokenizer: {PRIMARY_MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRIMARY_MODEL_NAME)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded successfully!\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  Model max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# ADDED: Set explicit max length for BERT models\n",
    "MAX_SEQUENCE_LENGTH = 512  # BERT's positional embedding limit\n",
    "\n",
    "print(f\"  Using max sequence length: {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Helper function to align labels with tokens\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Align BIO labels with tokenized inputs (handling subword tokenization)\n",
    "    \n",
    "    Args:\n",
    "        labels: Original labels (one per word)\n",
    "        word_ids: Word IDs from tokenizer (mapping tokens to words)\n",
    "    \n",
    "    Returns:\n",
    "        List of aligned labels\n",
    "    \"\"\"\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            # Special tokens get -100 (ignored in loss)\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            # First token of a word gets the label\n",
    "            aligned_labels.append(labels[word_idx])\n",
    "        else:\n",
    "            # Subsequent tokens of same word get -100\n",
    "            # Alternative: could use I- tag for continuation\n",
    "            aligned_labels.append(-100)\n",
    "        \n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "    return aligned_labels\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize inputs and align labels with subword tokens\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples from dataset\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    # BEFORE:\n",
    "    # tokenized_inputs = tokenizer(\n",
    "    #     examples['tokens'],\n",
    "    #     truncation=True,\n",
    "    #     is_split_into_words=True,\n",
    "    #     padding=False\n",
    "    # )\n",
    "    \n",
    "    # AFTER: Added explicit max_length parameter\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,  # ADDED: Explicit max length\n",
    "        is_split_into_words=True,\n",
    "        padding=False  # Will be handled by data collator\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        \n",
    "        # Convert string labels to IDs\n",
    "        label_ids = [tag2id[label] for label in labels]\n",
    "        \n",
    "        # Align labels with tokens\n",
    "        aligned_labels = align_labels_with_tokens(label_ids, word_ids)\n",
    "        all_labels.append(aligned_labels)\n",
    "    \n",
    "    tokenized_inputs['labels'] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Tokenizing datasets...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tokenize all splits\n",
    "tokenized_datasets = dataset_split.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_split['train'].column_names,\n",
    "    desc=\"Tokenizing datasets\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Tokenization complete!\")\n",
    "print(f\"\\nTokenized dataset structure:\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Test tokenization with one example\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Tokenized Example:\")\n",
    "print(\"=\"*80)\n",
    "sample = tokenized_datasets['train'][0]\n",
    "print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"Attention mask length: {len(sample['attention_mask'])}\")\n",
    "print(f\"Labels length: {len(sample['labels'])}\")\n",
    "print(f\"\\nFirst 10 input IDs: {sample['input_ids'][:10]}\")\n",
    "print(f\"First 10 labels: {sample['labels'][:10]}\")\n",
    "print(f\"First 10 tokens: {tokenizer.convert_ids_to_tokens(sample['input_ids'][:10])}\")\n",
    "\n",
    "# ADDED: Check for truncated sequences\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Checking for truncated sequences:\")\n",
    "print(f\"=\"*80)\n",
    "truncated_count = 0\n",
    "max_length_found = 0\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    for example in tokenized_datasets[split_name]:\n",
    "        length = len(example['input_ids'])\n",
    "        max_length_found = max(max_length_found, length)\n",
    "        if length >= MAX_SEQUENCE_LENGTH:\n",
    "            truncated_count += 1\n",
    "\n",
    "print(f\"  Max sequence length found: {max_length_found}\")\n",
    "print(f\"  Sequences at max length: {truncated_count}\")\n",
    "if truncated_count > 0:\n",
    "    print(f\"  ⚠ {truncated_count} sequences were truncated to {MAX_SEQUENCE_LENGTH} tokens\")\n",
    "else:\n",
    "    print(f\"  ✓ All sequences fit within {MAX_SEQUENCE_LENGTH} tokens\")\n",
    "\n",
    "# Create Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data collator created successfully!\")\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'tokenizer/vocab_size': len(tokenizer),\n",
    "    'tokenizer/model_max_length': tokenizer.model_max_length,\n",
    "    'tokenizer/used_max_length': MAX_SEQUENCE_LENGTH,  # ADDED\n",
    "    'tokenizer/truncated_sequences': truncated_count  # ADDED\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fddab",
   "metadata": {},
   "source": [
    "## 1.7 Evaluation Metrics Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.7 EVALUATION METRICS SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Setting up Evaluation Metrics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for NER task\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple of (predictions, labels)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted class (argmax of logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Convert IDs to tags, removing special tokens (label = -100)\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        true_prediction = []\n",
    "        \n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_label.append(id2tag[label_id])\n",
    "                true_prediction.append(id2tag[pred_id])\n",
    "        \n",
    "        if true_label:  # Only add if there are valid labels\n",
    "            true_labels.append(true_label)\n",
    "            true_predictions.append(true_prediction)\n",
    "    \n",
    "    # Compute metrics using seqeval\n",
    "    precision = seqeval_precision_score(true_labels, true_predictions)\n",
    "    recall = seqeval_recall_score(true_labels, true_predictions)\n",
    "    f1 = seqeval_f1_score(true_labels, true_predictions)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "# Helper function for tier-specific metrics\n",
    "def compute_metrics_for_tier(predictions, labels, tier_entities, id2tag):\n",
    "    \"\"\"\n",
    "    Compute metrics for a specific entity tier\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted labels\n",
    "        labels: True labels\n",
    "        tier_entities: List of entity types in this tier\n",
    "        id2tag: ID to tag mapping\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1\n",
    "    \"\"\"\n",
    "    # Filter predictions and labels for this tier\n",
    "    tier_pred = []\n",
    "    tier_true = []\n",
    "    \n",
    "    for pred_seq, true_seq in zip(predictions, labels):\n",
    "        tier_pred_seq = []\n",
    "        tier_true_seq = []\n",
    "        \n",
    "        for p, t in zip(pred_seq, true_seq):\n",
    "            if t == -100:  # Skip special tokens\n",
    "                continue\n",
    "                \n",
    "            pred_tag = id2tag.get(p, 'O')\n",
    "            true_tag = id2tag.get(t, 'O')\n",
    "            \n",
    "            # Check if entity type is in this tier\n",
    "            pred_entity_type = pred_tag.split('-')[1] if '-' in pred_tag else None\n",
    "            true_entity_type = true_tag.split('-')[1] if '-' in true_tag else None\n",
    "            \n",
    "            if true_entity_type in tier_entities or pred_entity_type in tier_entities:\n",
    "                tier_pred_seq.append(pred_tag)\n",
    "                tier_true_seq.append(true_tag)\n",
    "        \n",
    "        if tier_pred_seq:  # Only add if sequence has relevant entities\n",
    "            tier_pred.append(tier_pred_seq)\n",
    "            tier_true.append(tier_true_seq)\n",
    "    \n",
    "    if not tier_pred:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    # Compute metrics using seqeval\n",
    "    precision = seqeval_precision_score(tier_true, tier_pred)\n",
    "    recall = seqeval_recall_score(tier_true, tier_pred)\n",
    "    f1 = seqeval_f1_score(tier_true, tier_pred)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation metrics functions defined!\")\n",
    "print(\"\\nAvailable metric functions:\")\n",
    "print(\"  - compute_metrics(): Standard NER metrics (P/R/F1)\")\n",
    "print(\"  - compute_metrics_for_tier(): Tier-specific metrics\")\n",
    "print(\"\\nMetrics use seqeval library for proper BIO tag handling\")\n",
    "\n",
    "# Test metrics on a small sample\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Testing metrics with dummy data:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create dummy predictions and labels\n",
    "dummy_labels = [[tag2id['O'], tag2id['B-Disease_disorder'], tag2id['I-Disease_disorder'], tag2id['O']]]\n",
    "dummy_predictions = [[tag2id['O'], tag2id['B-Disease_disorder'], tag2id['I-Disease_disorder'], tag2id['O']]]\n",
    "\n",
    "# Convert to format expected by compute_metrics\n",
    "dummy_logits = np.zeros((1, 4, NUM_LABELS))\n",
    "for i, pred in enumerate(dummy_predictions[0]):\n",
    "    dummy_logits[0, i, pred] = 10.0  # High logit for predicted class\n",
    "\n",
    "test_metrics = compute_metrics((dummy_logits, np.array(dummy_labels)))\n",
    "print(f\"\\nTest metrics (perfect match):\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Metrics setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a5a43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195f687",
   "metadata": {},
   "source": [
    "# 2. Baseline Model: PubMedBERT Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8834c73",
   "metadata": {},
   "source": [
    "## 2.1 Model & Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.1 MODEL & TOKENIZER INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Initializing PubMedBERT Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model is already loaded from Section 1.6, but let's reinitialize for clarity\n",
    "print(f\"\\nModel: {PRIMARY_MODEL_NAME}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "# Initialize model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    PRIMARY_MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    "    ignore_mismatched_sizes=True  # In case of label mismatch\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Model Architecture Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Model type: {model.config.model_type}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Number of hidden layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Vocabulary size: {model.config.vocab_size}\")\n",
    "print(f\"  Max position embeddings: {model.config.max_position_embeddings}\")\n",
    "print(f\"  Number of labels: {model.config.num_labels}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Model Parameters:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"\\n✓ Model moved to: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print_gpu_memory()\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'model/total_parameters': total_params,\n",
    "    'model/trainable_parameters': trainable_params,\n",
    "    'model/hidden_size': model.config.hidden_size,\n",
    "    'model/num_layers': model.config.num_hidden_layers,\n",
    "    'model/device': str(device)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f6a45d",
   "metadata": {},
   "source": [
    "## 2.2 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e47688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.2 TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Setting up Training Configuration\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output directory\n",
    "    output_dir=MODEL_SAVE_DIR,\n",
    "\n",
    "    # Explicit run name for WandB\n",
    "    run_name=EXPERIMENT_NAME,  # Use our experiment name instead of output_dir\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    num_train_epochs=TRAINING_CONFIG['num_epochs'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay'],\n",
    "    warmup_steps=TRAINING_CONFIG['warmup_steps'],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "    \n",
    "    # Mixed precision training\n",
    "    fp16=TRAINING_CONFIG['fp16'],\n",
    "    \n",
    "    # Evaluation strategy\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=TRAINING_CONFIG['eval_steps'],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=TRAINING_CONFIG['save_steps'],\n",
    "    save_total_limit=TRAINING_CONFIG['save_total_limit'],\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=TRAINING_CONFIG['logging_steps'],\n",
    "    report_to=\"wandb\" if WANDB_ENABLED else \"none\",\n",
    "    \n",
    "    # Best model tracking\n",
    "    load_best_model_at_end=TRAINING_CONFIG['load_best_model_at_end'],\n",
    "    metric_for_best_model=TRAINING_CONFIG['metric_for_best_model'],\n",
    "    greater_is_better=TRAINING_CONFIG['greater_is_better'],\n",
    "    \n",
    "    # Other settings\n",
    "    push_to_hub=False,\n",
    "    seed=RANDOM_SEED,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training arguments configured!\")\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  FP16 training: {training_args.fp16}\")\n",
    "print(f\"  Evaluation steps: {training_args.eval_steps}\")\n",
    "print(f\"  Logging steps: {training_args.logging_steps}\")\n",
    "print(f\"  Save steps: {training_args.save_steps}\")\n",
    "print(f\"  Metric for best model: {training_args.metric_for_best_model}\")\n",
    "\n",
    "# Calculate training steps\n",
    "num_train_examples = len(tokenized_datasets['train'])\n",
    "steps_per_epoch = num_train_examples // (\n",
    "    training_args.per_device_train_batch_size * \n",
    "    training_args.gradient_accumulation_steps\n",
    ")\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Training Schedule:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Training examples: {num_train_examples}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Evaluations: ~{total_steps // training_args.eval_steps}\")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba8dc6",
   "metadata": {},
   "source": [
    "## 2.3 Training Loop with WandB Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf95f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.3 TRAINING LOOP WITH WANDB LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Starting Model Training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Record start time\n",
    "import time\n",
    "training_start_time = time.time()\n",
    "\n",
    "print(f\"\\nTraining started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print_gpu_memory()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Training in progress...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Record end time\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ Training completed successfully!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Display training results\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Total training time: {format_time(training_duration)}\")\n",
    "    print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"  Training steps: {train_result.global_step}\")\n",
    "    \n",
    "    # Get best metrics\n",
    "    best_metrics = trainer.state.best_metric\n",
    "    if best_metrics:\n",
    "        print(f\"  Best {training_args.metric_for_best_model}: {best_metrics:.4f}\")\n",
    "    \n",
    "    # GPU memory summary\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nGPU Memory Usage:\")\n",
    "        print_gpu_memory()\n",
    "        \n",
    "        # Get peak memory\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        print(f\"  Peak memory: {peak_memory:.2f}GB\")\n",
    "    \n",
    "    # Log training summary to WandB\n",
    "    log_to_wandb({\n",
    "        'training/duration_seconds': training_duration,\n",
    "        'training/final_loss': train_result.training_loss,\n",
    "        'training/total_steps': train_result.global_step,\n",
    "        'training/best_metric': best_metrics if best_metrics else 0.0\n",
    "    })\n",
    "    \n",
    "    # Save training metrics\n",
    "    training_metrics = {\n",
    "        'training_loss': float(train_result.training_loss),\n",
    "        'training_duration_seconds': training_duration,\n",
    "        'training_duration_formatted': format_time(training_duration),\n",
    "        'total_steps': train_result.global_step,\n",
    "        'best_metric': float(best_metrics) if best_metrics else None,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    save_results(training_metrics, 'training_summary.json')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"⚠ Training failed with error:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{str(e)}\")\n",
    "    raise e\n",
    "\n",
    "print(f\"\\n✓ Training phase complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da386c7e",
   "metadata": {},
   "source": [
    "## 2.4 Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.4 MODEL EVALUATION ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Evaluating Model on Test Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on validation set first\n",
    "print(\"\\n[1/2] Validation Set Evaluation:\")\n",
    "print(\"-\" * 80)\n",
    "val_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "for key, value in val_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\n[2/2] Test Set Evaluation:\")\n",
    "print(\"-\" * 80)\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create comparison table\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Metric': ['Precision', 'Recall', 'F1 Score'],\n",
    "    'Validation': [\n",
    "        val_results['eval_precision'],\n",
    "        val_results['eval_recall'],\n",
    "        val_results['eval_f1']\n",
    "    ],\n",
    "    'Test': [\n",
    "        test_results['eval_precision'],\n",
    "        test_results['eval_recall'],\n",
    "        test_results['eval_f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Results Comparison:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(results_comparison.to_string(index=False))\n",
    "\n",
    "# Calculate performance difference\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Performance Analysis:\")\n",
    "print(f\"{'='*80}\")\n",
    "f1_diff = test_results['eval_f1'] - val_results['eval_f1']\n",
    "print(f\"  F1 difference (Test - Val): {f1_diff:+.4f}\")\n",
    "\n",
    "if abs(f1_diff) < 0.02:\n",
    "    print(f\"  Status: ✓ Good generalization (difference < 2%)\")\n",
    "elif abs(f1_diff) < 0.05:\n",
    "    print(f\"  Status: ⚠ Moderate difference (2-5%)\")\n",
    "else:\n",
    "    print(f\"  Status: ⚠ Significant difference (>5%) - possible overfitting/underfitting\")\n",
    "\n",
    "# Log to WandB\n",
    "log_to_wandb({\n",
    "    'val/precision': val_results['eval_precision'],\n",
    "    'val/recall': val_results['eval_recall'],\n",
    "    'val/f1': val_results['eval_f1'],\n",
    "    'test/precision': test_results['eval_precision'],\n",
    "    'test/recall': test_results['eval_recall'],\n",
    "    'test/f1': test_results['eval_f1'],\n",
    "    'test/f1_diff_from_val': f1_diff\n",
    "})\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'validation': {\n",
    "        'precision': float(val_results['eval_precision']),\n",
    "        'recall': float(val_results['eval_recall']),\n",
    "        'f1': float(val_results['eval_f1']),\n",
    "        'loss': float(val_results['eval_loss'])\n",
    "    },\n",
    "    'test': {\n",
    "        'precision': float(test_results['eval_precision']),\n",
    "        'recall': float(test_results['eval_recall']),\n",
    "        'f1': float(test_results['eval_f1']),\n",
    "        'loss': float(test_results['eval_loss'])\n",
    "    },\n",
    "    'comparison': {\n",
    "        'f1_difference': float(f1_diff),\n",
    "        'generalization_quality': 'good' if abs(f1_diff) < 0.02 else 'moderate' if abs(f1_diff) < 0.05 else 'poor'\n",
    "    }\n",
    "}\n",
    "\n",
    "save_results(evaluation_results, 'evaluation_results.json')\n",
    "\n",
    "# Generate predictions for detailed analysis\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Generating predictions for analysis...\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions_output = trainer.predict(tokenized_datasets['test'])\n",
    "predictions = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "\n",
    "# Convert to predicted class\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "print(f\"\\n✓ Predictions generated!\")\n",
    "print(f\"  Shape: {predictions.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "\n",
    "# Store predictions for later analysis\n",
    "test_predictions = {\n",
    "    'predictions': predictions,\n",
    "    'labels': labels,\n",
    "    'metrics': predictions_output.metrics\n",
    "}\n",
    "\n",
    "# Save predictions (as numpy arrays)\n",
    "np.save(f\"{RESULTS_DIR}/test_predictions.npy\", predictions)\n",
    "np.save(f\"{RESULTS_DIR}/test_labels.npy\", labels)\n",
    "\n",
    "print(f\"\\n✓ Predictions saved to {RESULTS_DIR}/\")\n",
    "print(f\"\\n✓ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5b47e",
   "metadata": {},
   "source": [
    "## 2.5 Save Model Artifacts to WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.5 SAVE MODEL ARTIFACTS TO WANDB\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Saving Model Artifacts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model and tokenizer locally\n",
    "print(\"\\n[1/4] Saving model and tokenizer locally...\")\n",
    "model_save_path = f\"{MODEL_SAVE_DIR}/best_model\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"✓ Model saved to: {model_save_path}\")\n",
    "\n",
    "# Save configuration files\n",
    "print(\"\\n[2/4] Saving configuration files...\")\n",
    "\n",
    "# Save tag mappings\n",
    "mappings_save_path = f\"{model_save_path}/tag_mappings.json\"\n",
    "with open(mappings_save_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'tag2id': tag2id,\n",
    "        'id2tag': id2tag,\n",
    "        'num_labels': NUM_LABELS\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"✓ Tag mappings saved to: {mappings_save_path}\")\n",
    "\n",
    "# Save training configuration\n",
    "config_save_path = f\"{model_save_path}/training_config.json\"\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'model_name': PRIMARY_MODEL_NAME,\n",
    "        'dataset_name': DATASET_NAME,\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'entity_tiers': ENTITY_TIERS,\n",
    "        'high_value_entities': HIGH_VALUE_ENTITIES,\n",
    "        'remove_rare_entities': REMOVE_RARE_ENTITIES,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'timestamp': TIMESTAMP\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"✓ Training config saved to: {config_save_path}\")\n",
    "\n",
    "# Create model card\n",
    "print(\"\\n[3/4] Creating model card...\")\n",
    "model_card = f\"\"\"\n",
    "# {MODEL_CONFIGS[PRIMARY_MODEL]['display_name']} for Medical NER\n",
    "\n",
    "## Model Information\n",
    "- **Base Model**: {PRIMARY_MODEL_NAME}\n",
    "- **Task**: Named Entity Recognition (Token Classification)\n",
    "- **Dataset**: {DATASET_NAME}\n",
    "- **Number of Labels**: {NUM_LABELS}\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Training Configuration\n",
    "- Learning Rate: {TRAINING_CONFIG['learning_rate']}\n",
    "- Batch Size: {TRAINING_CONFIG['batch_size']}\n",
    "- Epochs: {TRAINING_CONFIG['num_epochs']}\n",
    "- Warmup Steps: {TRAINING_CONFIG['warmup_steps']}\n",
    "- Weight Decay: {TRAINING_CONFIG['weight_decay']}\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "# Validation Set\n",
    "- Precision: {val_results['eval_precision']:.4f}\n",
    "- Recall: {val_results['eval_recall']:.4f}\n",
    "- F1 Score: {val_results['eval_f1']:.4f}\n",
    "\n",
    "# Test Set\n",
    "- Precision: {test_results['eval_precision']:.4f}\n",
    "- Recall: {test_results['eval_recall']:.4f}\n",
    "- F1 Score: {test_results['eval_f1']:.4f}\n",
    "\n",
    "## Entity Types\n",
    "- Total Entity Types: {len(entity_df)}\n",
    "- Frequent Entities: {len(entity_df[entity_df['Tier'] == 'frequent'])}\n",
    "- Medium Entities: {len(entity_df[entity_df['Tier'] == 'medium'])}\n",
    "- Rare Entities: {len(entity_df[entity_df['Tier'] == 'rare'])}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"{model_save_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_save_path}\")\n",
    "\n",
    "# Example inference\n",
    "text = \"Patient is a 45-year-old male with diabetes.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Rare entities {'removed' if REMOVE_RARE_ENTITIES else 'included'}\n",
    "- Random seed: {RANDOM_SEED} \"\"\"\n",
    "\n",
    "model_card_path = f\"{model_save_path}/MODEL_CARD.md\" \n",
    "with open(model_card_path, 'w') as f: f.write(model_card)\n",
    "\n",
    "print(f\"✓ Model card saved to: {model_card_path}\")\n",
    "\n",
    "# Upload to WandB\n",
    "\n",
    "if WANDB_ENABLED: print(\"\\n[4/4] Uploading artifacts to WandB...\")\n",
    "try:\n",
    "    # Create model artifact\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=f\"pubmedbert-ner-{TIMESTAMP}\",\n",
    "        type=\"model\",\n",
    "        description=f\"Fine-tuned {MODEL_CONFIGS[PRIMARY_MODEL]['display_name']} for Medical NER\",\n",
    "        metadata={\n",
    "            'model_name': PRIMARY_MODEL_NAME,\n",
    "            'test_f1': float(test_results['eval_f1']),\n",
    "            'test_precision': float(test_results['eval_precision']),\n",
    "            'test_recall': float(test_results['eval_recall']),\n",
    "            'num_labels': NUM_LABELS\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add model directory\n",
    "    model_artifact.add_dir(model_save_path)\n",
    "    \n",
    "    # Log artifact\n",
    "    wandb.log_artifact(model_artifact)\n",
    "    \n",
    "    print(f\"✓ Model artifact uploaded to WandB\")\n",
    "    \n",
    "    # Create results artifact\n",
    "    results_artifact = wandb.Artifact(\n",
    "        name=f\"results-{TIMESTAMP}\",\n",
    "        type=\"results\",\n",
    "        description=\"Training and evaluation results\"\n",
    "    )\n",
    "        \n",
    "    # Add results directory\n",
    "    results_artifact.add_dir(RESULTS_DIR)\n",
    "    \n",
    "    # Log artifact\n",
    "    wandb.log_artifact(results_artifact)\n",
    "    \n",
    "    print(f\"✓ Results artifact uploaded to WandB\")\n",
    "    \n",
    "    # Create plots artifact\n",
    "    plots_artifact = wandb.Artifact(\n",
    "        name=f\"plots-{TIMESTAMP}\",\n",
    "        type=\"plots\",\n",
    "        description=\"Visualizations and plots\"\n",
    "    )\n",
    "    \n",
    "    # Add plots directory\n",
    "    plots_artifact.add_dir(PLOTS_DIR)\n",
    "    \n",
    "    # Log artifact\n",
    "    wandb.log_artifact(plots_artifact)\n",
    "    \n",
    "    print(f\"✓ Plots artifact uploaded to WandB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Failed to upload to WandB: {e}\")\n",
    "\n",
    "else: print(\"\\n[4/4] WandB not enabled - skipping upload\")\n",
    "\n",
    "print(f\"\\n{'='*80}\") \n",
    "print(\"✓ All artifacts saved successfully!\") \n",
    "print(f\"{'='*80}\") \n",
    "print(f\"\\nLocal paths:\") \n",
    "print(f\" Model: {model_save_path}\") \n",
    "print(f\" Results: {RESULTS_DIR}\") \n",
    "print(f\" Plots: {PLOTS_DIR}\")\n",
    "\n",
    "if WANDB_ENABLED: print(f\"\\nWandB Dashboard: {wandb.run.get_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09184fef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cefb4",
   "metadata": {},
   "source": [
    "# 3. Additional Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f78e33",
   "metadata": {},
   "source": [
    "## 3.1 BioBERT Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9076d63",
   "metadata": {},
   "source": [
    "## 3.2 Alternative Model Training (Bioformer/ClinicalBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e9bcd",
   "metadata": {},
   "source": [
    "## 3.3 Model Comparison Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747f70d",
   "metadata": {},
   "source": [
    "## 3.4 Best Model Selection Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d009319",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f54cf5",
   "metadata": {},
   "source": [
    "# 4. Per-Entity-Type Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faef815",
   "metadata": {},
   "source": [
    "## 4.1 Extract Predictions by Entity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879b9ad",
   "metadata": {},
   "source": [
    "## 4.2 Calculate Metrics per Entity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601613c8",
   "metadata": {},
   "source": [
    "## 4.3 Tier-Based Analysis (Frequent/Medium/Rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da50e0",
   "metadata": {},
   "source": [
    "## 4.4 Identify Systematic Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198205ac",
   "metadata": {},
   "source": [
    "## 4.5 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66557fa8",
   "metadata": {},
   "source": [
    "### 4.5.1 F1 Score by Entity Type (Bar Chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96924a3",
   "metadata": {},
   "source": [
    "### 4.5.2 Entity Frequency vs F1 Score (Scatter Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7aa64",
   "metadata": {},
   "source": [
    "### 4.5.3 Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da6e09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bd6f3",
   "metadata": {},
   "source": [
    "# 5. Error Analysis Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdf5de",
   "metadata": {},
   "source": [
    "## 5.1 Boundary Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732d02b",
   "metadata": {},
   "source": [
    "### 5.1.1 Partial Match Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8207bf1",
   "metadata": {},
   "source": [
    "### 5.1.2 Exact Match vs Partial Match Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f8b29",
   "metadata": {},
   "source": [
    "## 5.2 Entity Type Confusion Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848735",
   "metadata": {},
   "source": [
    "### 5.2.1 Build Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e60865",
   "metadata": {},
   "source": [
    "### 5.2.2 Common Confusion Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23325f38",
   "metadata": {},
   "source": [
    "## 5.3 Multi-token Entity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee1923",
   "metadata": {},
   "source": [
    "### 5.3.1 Single-token vs Multi-token Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0226a",
   "metadata": {},
   "source": [
    "### 5.3.2 Entity Length Impact on F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3625d0",
   "metadata": {},
   "source": [
    "## 5.4 Error Examples Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504bf77",
   "metadata": {},
   "source": [
    "### 5.4.1 Sample Mispredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb98344",
   "metadata": {},
   "source": [
    "### 5.4.2 Error Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823d363",
   "metadata": {},
   "source": [
    "### 5.4.3 Pattern Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbc6c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c531c90e",
   "metadata": {},
   "source": [
    "# 6. Confidence Score Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d4751",
   "metadata": {},
   "source": [
    "## 6.1 Extract Prediction Confidence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430df535",
   "metadata": {},
   "source": [
    "## 6.2 Confidence Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b1406",
   "metadata": {},
   "source": [
    "### 6.2.1 Correct vs Incorrect Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c1354",
   "metadata": {},
   "source": [
    "### 6.2.2 Average Confidence by Prediction Type (TP/FP/FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd903034",
   "metadata": {},
   "source": [
    "## 6.3 Threshold Analysis for Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd84f16",
   "metadata": {},
   "source": [
    "### 6.3.1 Test Multiple Thresholds [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082412d",
   "metadata": {},
   "source": [
    "### 6.3.2 Precision/Coverage/Recall at Each Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609c9ea",
   "metadata": {},
   "source": [
    "## 6.4 Precision-Coverage Tradeoff Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b5fe1",
   "metadata": {},
   "source": [
    "## 6.5 Production Threshold Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e9ffe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7ff8a",
   "metadata": {},
   "source": [
    "# 7. Inference Speed & Scalability Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fffe5",
   "metadata": {},
   "source": [
    "## 7.1 Measure Inference Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0336d",
   "metadata": {},
   "source": [
    "### 7.1.1 Per Document (Batch Size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465d725",
   "metadata": {},
   "source": [
    "### 7.1.2 Per Batch (Batch Size = 8, 16, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed1e13",
   "metadata": {},
   "source": [
    "## 7.2 Memory Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefdfd2",
   "metadata": {},
   "source": [
    "### 7.2.1 GPU Memory During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3bd04",
   "metadata": {},
   "source": [
    "### 7.2.2 GPU Memory During Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffee15",
   "metadata": {},
   "source": [
    "### 7.2.3 CPU Inference Feasibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428aac09",
   "metadata": {},
   "source": [
    "## 7.3 Throughput Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6817c5",
   "metadata": {},
   "source": [
    "### 7.3.1 Documents per Second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf448cd",
   "metadata": {},
   "source": [
    "### 7.3.2 Large-scale Processing Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ab4c7",
   "metadata": {},
   "source": [
    "## 7.4 Scalability Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1644ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46374a1",
   "metadata": {},
   "source": [
    "# 8. Production-Readiness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0206c",
   "metadata": {},
   "source": [
    "## 8.1 Clinical Coding Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94399f3a",
   "metadata": {},
   "source": [
    "### 8.1.1 Auto-code vs Manual Review Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496413a6",
   "metadata": {},
   "source": [
    "### 8.1.2 Automation Rate at Different Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e562a",
   "metadata": {},
   "source": [
    "## 8.2 High-Value Entity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb56776",
   "metadata": {},
   "source": [
    "### 8.2.1 F1 on Critical Entities (Disease_disorder, Therapeutic_procedure, Medication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1de0ec",
   "metadata": {},
   "source": [
    "### 8.2.2 Precision on High-Value Entities at 0.85 Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88085ae",
   "metadata": {},
   "source": [
    "## 8.3 False Positive Impact Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aada7",
   "metadata": {},
   "source": [
    "### 8.3.1 FP Rate by Entity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62bdae",
   "metadata": {},
   "source": [
    "### 8.3.2 High-Risk Entity Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebee592",
   "metadata": {},
   "source": [
    "## 8.4 Production Readiness Report Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bc6cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53ca15",
   "metadata": {},
   "source": [
    "# 9. Model Selection & Final Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89dfee",
   "metadata": {},
   "source": [
    "## 9.1 Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5721c",
   "metadata": {},
   "source": [
    "### 9.1.1 Selection Criteria & Tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d2b6b",
   "metadata": {},
   "source": [
    "### 9.1.2 Justification Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2699c98",
   "metadata": {},
   "source": [
    "## 9.2 Phase 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d26207",
   "metadata": {},
   "source": [
    "### 9.2.1 Training Results Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff429800",
   "metadata": {},
   "source": [
    "### 9.2.2 Key Visualizations Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc0105",
   "metadata": {},
   "source": [
    "### 9.2.3 Error Analysis Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a27afc",
   "metadata": {},
   "source": [
    "### 9.2.4 Production Metrics Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d89345",
   "metadata": {},
   "source": [
    "## 9.3 Model Artifacts Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01330e",
   "metadata": {},
   "source": [
    "### 9.3.1 Save Best Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cafdfc9",
   "metadata": {},
   "source": [
    "### 9.3.2 Save Tokenizer & Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aad4d7",
   "metadata": {},
   "source": [
    "### 9.3.3 Save Evaluation Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f890a5",
   "metadata": {},
   "source": [
    "## 9.4 Phase 1 Insights Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b01b2",
   "metadata": {},
   "source": [
    "### 9.4.1 Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae0458",
   "metadata": {},
   "source": [
    "### 9.4.2 Surprising Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5ea49",
   "metadata": {},
   "source": [
    "### 9.4.3 Recommendations for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb08e38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd046c",
   "metadata": {},
   "source": [
    "# 10. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d13a12",
   "metadata": {},
   "source": [
    "## 10.1 Utility Functions Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d27c46",
   "metadata": {},
   "source": [
    "## 10.2 Hyperparameter Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f168207",
   "metadata": {},
   "source": [
    "## 10.3 Raw Results Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17445701",
   "metadata": {},
   "source": [
    "## 10.4 Additional Visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

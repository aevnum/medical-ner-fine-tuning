{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [{'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 6, 82, 82, 23, 82, 82, 82, 34, 82, 82, 5, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 11, 52, 52, 52, 52, 82, 11, 52, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 19, 82, 19, 60, 60, 82, 82, 19, 60, 82, 82, 11, 52, 82, 82, 82, 34, 82, 13, 54, 82, 82, 82, 82, 82, 5, 46, 46, 82, 82, 34, 5, 46, 46, 82, 82, 82, 82, 11, 82, 82, 13, 54, 54, 54, 82, 82, 34, 82, 10, 51, 82, 82, 5, 46, 46, 82, 82, 82, 82, 82, 34, 82, 13, 54, 54, 54, 82, 82, 82, 82, 82, 34, 5, 46, 46, 46, 82, 11, 52, 52, 82, 20, 82, 82, 82, 82, 34, 75, 82, 82, 11, 82, 20, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 82, 20, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 34, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 9, 50, 50, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 6, 82, 82, 23, 64, 82, 82, 82, 34, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 11, 52, 82, 82, 34, 82, 34, 75, 82, 82, 5, 82, 11, 52, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 20, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 5, 46, 82, 5, 46, 82, 82, 5, 46, 82, 82, 37, 82, 82, 5, 46, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 20, 61, 61, 61, 9, 50, 50, 50, 82, 11, 52, 82, 82, 20, 82, 11, 52, 82, 82, 12, 82, 82, 5, 46, 46, 82, 82, 82, 5, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 22, 82, 82, 37, 78, 82, 82, 82, 82, 82, 34, 82, 34, 9, 50, 50, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 11, 52, 82, 13, 54, 82, 20, 82, 11, 52, 52, 82, 13, 54, 82, 82, 82, 11, 52, 52, 52, 52, 82, 13, 54, 82, 82, 82, 12, 82, 82, 82, 82, 82, 5, 46, 46, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 34, 75, 82, 11, 52, 52, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 5, 46, 82, 20, 82, 82, 5, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 13, 54, 54, 54, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 9, 50, 50, 50, 82, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 34, 82, 34, 82, 82, 34, 82, 82, 82, 82, 23, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 82, 11, 52, 52, 82, 82, 82, 12, 53, 82, 82, 82, 5, 46, 82, 82, 5, 46, 82, 12, 53, 53, 82, 82, 5, 82, 82, 11, 82, 12, 82, 82, 9, 50, 50, 50, 82, 82, 37, 82, 37, 78, 78, 78, 78, 78, 82, 37, 78, 78, 78, 78, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 11, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 82, 11, 52, 52, 82, 82, 20, 82, 82, 11, 52, 52, 52, 82, 11, 82, 82, 20, 82, 82, 11, 82, 20, 82, 82, 11, 52, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 20, 82, 82, 11, 52, 52, 52, 52, 82, 11, 52, 52, 82, 82, 20, 61, 61, 82, 82, 11, 52, 52, 52, 82, 11, 52, 52, 82, 82, 11, 82, 20, 82, 82, 82, 11, 52, 52, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 29, 5, 46, 46, 12, 82, 31, 72, 72, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 82, 22, 82, 22, 63, 63, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 9, 50, 50, 50, 82, 37, 78, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 82, 11, 52, 52, 82, 82, 31, 10, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 82, 82, 5, 46, 82, 82, 5, 46, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 11, 82, 82, 82, 82, 20, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 1, 22, 63, 14, 55, 17, 58, 58, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 78, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 14, 55, 55, 55, 55, 82, 82, 5, 46, 82, 82, 82, 14, 55, 55, 55, 55, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 82, 82, 82, 22, 63, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 9, 50, 50, 50, 82, 82, 20, 61, 61, 82, 82, 11, 52, 52, 52, 82, 11, 82, 82, 11, 52, 52, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 28, 32, 82, 82, 82, 34, 75, 82, 34, 82, 82, 82, 34, 9, 50, 50, 50, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 20, 61, 61, 17, 58, 82, 82, 82, 82, 11, 52, 82, 82, 82, 23, 64, 64, 82, 23, 82, 9, 50, 50, 50, 82, 82, 20, 82, 82, 82, 82, 82, 11, 52, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, 11, 52, 52, 52, 82, 82, 11, 52, 82, 82, 82, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 23, 64, 64, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 82, 82, 22, 82, 82, 82, 6, 82, 82, 23, 82, 9, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 82, 22, 63, 82, 14, 55, 55, 55, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 49, 49, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 82, 22, 63, 63, 63, 63, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 18, 59, 82, 82, 82, 82, 82, 40, 81, 82, 82, 11, 52, 52, 82, 20, 61, 61, 61, 82, 82, 11, 52, 82, 20, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 61, 82, 82, 11, 52, 52, 52, 52, 82, 11, 82, 82, 20, 61, 82, 82, 82, 82, 82, 34, 75, 75, 75, 82, 34, 75, 75, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 34, 75, 82, 82, 5, 46, 82, 5, 46, 46, 46, 46, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 82, 82, 11, 52, 82, 20, 82, 82, 82, 82, 82, 82, 82, 11, 82, 20, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 52, 82, 11, 52, 52, 52, 82, 82, 82, 20, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 11, 52, 52, 52, 82, 20, 61, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 61, 61, 82, 11, 52, 52, 52, 52, 52, 82, 20, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 12, 53, 53, 53, 53, 82, 12, 82, 82, 9, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 20, 82, 82, 82, 82, 82, 22, 63, 63, 63, 22, 63, 63, 63, 63, 63, 63, 14, 55, 55, 82, 9, 50, 82, 22, 63, 63, 22, 63, 63, 63, 63, 14, 55, 55, 82, 9, 50, 82, 82, 82, 9, 50, 50, 50, 82, 82, 6, 82, 82, 23, 64, 82, 31, 34, 75, 82, 34, 75, 75, 82, 34, 82, 82, 34, 75, 82, 82, 6, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 82, 11, 52, 52, 82, 82, 11, 82, 20, 61, 61, 82, 11, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 20, 61, 82, 82, 5, 11, 52, 52, 82, 82, 34, 75, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 5, 34, 82, 82, 82, 82, 12, 53, 53, 82, 82, 82, 82, 22, 63, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 11, 52, 82, 20, 82, 82, 82, 6, 82, 82, 23, 82, 82, 82, 82, 82, 6, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 82, 82, 22, 82, 82, 11, 52, 82, 20, 61, 61, 82, 82, 11, 82, 20, 61, 61, 61, 61, 82, 82, 82, 11, 52, 52, 34, 75, 75, 75, 75, 82, 11, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 13, 54, 82, 82, 5, 46, 82, 82, 20, 61, 61, 82, 82, 11, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 82, 82, 11, 52, 52, 52, 52, 82, 82, 20, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 37, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 8, 82, 11, 52, 20, 82, 82, 11, 52, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 82, 11, 52, 82, 20, 61, 61, 61, 61, 82, 82, 34, 75, 82, 11, 52, 52, 82, 82, 20, 61, 82, 82, 82, 82, 82, 82, 12, 82, 34, 75, 75, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 8, 82, 22, 63, 63, 82, 22, 63, 63, 63, 82, 82, 82, 11, 52, 52, 52, 82, 11, 52, 82, 82, 11, 82, 11, 52, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 23, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 82, 82, 22, 82, 22, 63, 63, 1, 82, 82, 82, 82, 82, 37, 78, 78, 78, 78, 78, 82, 37, 78, 82, 82, 82, 82, 8, 82, 82, 82, 22, 63, 63, 63, 82, 11, 52, 82, 20, 82, 82, 82, 82, 82, 11, 82, 82, 5, 46, 82, 82, 82, 82, 82, 12, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 6, 82, 82, 23, 82, 82, 82, 34, 75, 82, 82, 82, 11, 52, 52, 82, 82, 82, 19, 82, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 60, 60, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 11, 52, 82, 11, 52, 82, 20, 82, 11, 82, 20, 61, 10, 51, 82, 82, 82, 11, 52, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 34, 75, 75, 82, 82, 5, 46, 46, 46, 46, 82, 82, 82, 82, 82, 11, 52, 82, 20, 82, 11, 52, 82, 82, 82, 82, 11, 52, 52, 82, 11, 52, 82, 11, 82, 82, 82, 11, 52, 52, 52, 52, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 10, 51, 82, 9, 50, 50, 50, 50, 82, 82, 5, 46, 37, 78, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 46, 82, 82, 34, 82, 34, 75, 82, 82, 34, 82, 5, 46, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 11, 52, 82, 82, 82, 82, 20, 61, 82, 82, 20, 61, 61, 61, 61, 61, 61, 61, 61, 82, 82, 11, 52, 82, 20, 82, 82, 11, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 12, 53, 53, 53, 53, 53, 82, 12, 82, 82, 82, 82, 82, 11, 82, 11, 82, 82, 82, 11, 82, 11, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 82, 11, 52, 52, 52, 52, 82, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 82, 82, 11, 52, 82, 11, 82, 82, 20, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 14, 55, 55, 55, 55, 55, 55, 55, 55, 55, 82, 82, 82, 82, 82, 34, 75, 82, 9, 50, 50, 82, 82, 82, 34, 82, 82, 82, 82, 11, 82, 82, 20, 9, 50, 50, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 6, 82, 82, 23, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 34, 82, 82, 34, 75, 75, 82, 34, 75, 75, 75, 82, 34, 75, 75, 82, 82, 34, 82, 82, 82, 82, 19, 60, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 34, 82, 34, 75, 75, 75, 82, 34, 82, 82, 34, 82, 82, 5, 82, 11, 52, 82, 20, 82, 11, 52, 52, 82, 82, 11, 82, 82, 82, 11, 20, 82, 82, 82, 11, 52, 52, 52, 52, 82, 20, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 39, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 82, 5, 46, 46, 46, 82, 10, 51, 51, 82, 82, 82, 82, 82, 82, 11, 82, 82, 34, 82, 82, 5, 82, 5, 82, 5, 46, 82, 82, 5, 46, 82, 11, 52, 82, 82, 82, 34, 82, 82, 5, 82, 82, 13, 54, 54, 54, 54, 54, 54, 82, 5, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 10, 51, 82, 82, 8, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 12, 53, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 8, 82, 82, 37, 82, 82, 82, 37, 82, 82, 37, 78, 82, 37, 78, 82, 20, 82, 82, 82, 82, 82, 82, 34, 75, 82, 39, 80, 80, 80, 80, 80, 80, 80, 80, 80, 82, 82, 82, 82, 82, 82, 5, 82, 82, 82, 82, 82, 5, 46, 82, 82, 82, 82, 5, 82, 82, 8, 49, 82, 82, 82, 5, 46, 82, 10, 51, 51, 51, 82, 82, 34, 82, 12, 82, 82, 12, 82, 82, 82, 82, 82, 7, 48, 48, 82, 7, 48, 48, 48, 82, 82, 82, 11, 52, 82, 11, 82, 11, 52, 82, 82, 8, 82, 10, 51, 51, 51, 51, 82, 82, 5, 46, 46, 46, 82, 82, 82, 82, 82, 5, 46, 82, 82, 8, 82, 10, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 82, 10, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 10, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 82, 82, 10, 51, 82, 10, 51, 82, 10, 51, 82, 10, 51, 82, 82, 10, 51, 82, 82, 82, 82, 82, 82, 11, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 19, 82, 19, 6, 82, 82, 23, 64, 82, 82, 15, 56, 56, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 82, 5, 82, 82, 82, 82, 82, 34, 82, 34, 75, 82, 82, 82, 34, 75, 82, 34, 75, 82, 11, 52, 82, 20, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 52, 82, 82, 82, 20, 61, 82, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 82, 5, 46, 46, 46, 82, 82, 82, 82, 34, 75, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 5, 82, 82, 5, 46, 82, 82, 82, 6, 82, 82, 11, 52, 52, 52, 82, 37, 82, 82, 82, 82, 82, 12, 82, 9, 50, 82, 82, 82, 82, 22, 63, 82, 82, 82, 82, 82, 0, 41, 82, 15, 56, 56, 56, 56, 82, 12, 53, 53, 53, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 82, 82, 12, 82, 11, 82, 82, 82, 82, 5, 46, 82, 82, 82, 5, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 61, 61, 82, 82, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 11, 82, 11, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 12, 53, 53, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 0, 82, 0, 41, 82, 34, 75, 75, 82, 82, 82, 82, 82, 5, 46, 82, 0, 41, 82, 82, 82, 82, 34, 82, 82, 5, 46, 46, 82, 82, 82, 82, 0, 82, 0, 41, 82, 11, 82, 82, 5, 82, 34, 82, 82, 5, 46, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 34, 82, 82, 12, 53, 53, 82, 82, 34, 82, 82, 82, 82, 5, 46, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 34, 75, 75, 75, 75, 82, 82, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 12, 53, 53, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 6, 82, 82, 23, 82, 82, 15, 56, 56, 82, 82, 82, 34, 75, 82, 11, 52, 82, 82, 82, 34, 82, 82, 5, 46, 82, 82, 11, 52, 52, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 82, 11, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 10, 82, 20, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 20, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 13, 54, 82, 82, 5, 46, 82, 82, 82, 82, 34, 82, 34, 75, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 34, 82, 82, 34, 82, 82, 5, 46, 82, 82, 11, 52, 82, 11, 52, 82, 82, 8, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 34, 82, 82, 34, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 34, 75, 82, 34, 75, 75, 75, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 37, 82, 82, 82, 82, 82, 11, 52, 82, 82, 8, 82, 82, 5, 46, 82, 34, 82, 5, 46, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 82, 34, 82, 5, 34, 82, 82, 82, 82, 5, 46, 46, 46, 82, 82, 82, 82, 39, 80, 80, 82, 82, 34, 75, 75, 82, 37, 78, 82, 11, 52, 52, 82, 82, 34, 82, 82, 82, 82, 34, 82, 34, 75, 82, 82, 11, 52, 82, 82, 37, 78, 78, 78, 82, 82, 37, 82, 37, 78, 78, 82, 82, 37, 78, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 34, 82, 82, 82, 5, 82, 82, 5, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 11, 82, 34, 75, 82, 82, 5, 82, 82, 5, 82, 82, 82, 82, 82, 11, 52, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 20, 82, 82, 82, 34, 75, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 82, 22, 63, 63, 82, 82, 22, 63, 63, 82, 82, 22, 82, 22, 63, 82, 82, 82, 17, 58, 58, 58, 58, 58, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 28, 32, 82, 82, 34, 82, 15, 56, 82, 82, 6, 82, 82, 23, 82, 82, 82, 82, 34, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 60, 60, 82, 82, 82, 82, 82, 82, 22, 63, 63, 63, 63, 82, 82, 82, 82, 82, 34, 82, 16, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 82, 5, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 46, 82, 5, 46, 46, 46, 46, 82, 5, 46, 46, 46, 46, 46, 82, 82, 5, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 82, 5, 46, 46, 46, 82, 34, 75, 75, 82, 82, 11, 52, 52, 52, 82, 82, 82, 34, 75, 82, 82, 11, 52, 52, 52, 82, 82, 82, 82, 82, 82, 82, 10, 51, 51, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 52, 82, 11, 52, 52, 52, 82, 82, 11, 52, 52, 82, 82, 11, 52, 52, 82, 11, 82, 20, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 20, 61, 61, 61, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 82, 82, 82, 20, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 11, 52, 52, 82, 82, 82, 11, 52, 82, 82, 11, 52, 82, 82, 82, 82, 20, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 14, 55, 55, 82, 82, 9, 50, 50, 50, 50, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 8, 82, 7, 82, 10, 82, 82, 82, 10, 51, 51, 82, 82, 82, 82, 82, 36, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 1, 42, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 9, 50, 50, 82, 82, 82, 82, 6, 82, 82, 82, 23, 82, 82, 82, 82, -100]}, {'labels': [-100, 9, 50, 82, 82, 2, 43, 43, 43, 43, 32, 82, 6, 23, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 82, 82, 9, 50, 50, 50, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 14, 55, 82, 10, 22, 82, 22, 63, 82, 82, 82, 82, 22, 82, 22, 82, 82, 22, 82, 82, 82, 82, 82, 9, 50, 82, 82, 34, 82, 82, 82, 82, 5, 46, 46, 46, 82, 82, 34, 82, 82, 82, 11, 82, 9, 50, 50, 50, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 37, 78, 82, 82, 82, 6, 47, 47, 47, 47, 47, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 9, 50, 50, 50, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 34, 75, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 9, 50, 50, 50, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 22, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 82, 82, 82, 11, 52, 52, 52, 52, 82, 82, 82, 82, 82, 82, 22, 63, 82, 82, 82, 82, 82, 82, 14, 55, 55, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 82, 37, 78, 82, 22, 63, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 63, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 14, 55, 55, 55, 82, 82, 82, 82, 82, 82, 22, 63, 63, 63, 82, 14, 55, 55, 55, 55, 55, 82, 82, 82, 82, 37, 82, 82, 82, 82, 37, 78, 78, 78, 82, 10, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 46, 46, 46, 82, 82, 12, 82, 82, 82, 37, 78, 82, 8, 82, 82, 82, 82, 82, 8, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 78, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 5, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 1, 42, 42, 42, 82, 22, 63, 63, 63, 82, 1, 42, 14, 55, 55, 55, 55, 55, 55, 55, 82, 82, 34, 75, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 78, 82, 82, 10, 51, 51, 51, 82, 82, 82, 10, 51, 51, 82, 82, 82, 82, 82, 82, 15, 56, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 34, 75, 82, 34, 75, 75, 82, 82, 82, 34, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 34, 82, 34, 75, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 82, 82, 82, 82, 11, 52, 82, 82, 82, 11, 52, 52, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 22, 82, 22, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 9, 50, 50, 50, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 12, 53, 53, 53, 53, 82, 12, 82, 82, 9, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 20, 82, 82, 82, 82, 82, 22, 63, 63, 63, 22, 63, 63, 63, 63, 63, 63, 14, 55, 55, 82, 9, 50, 82, 22, 63, 63, 22, 63, 63, 63, 63, 14, 55, 55, 82, 9, 50, 82, 82, 82, 9, 50, 50, 50, 82, 82, 6, 82, 82, 23, 64, 82, 31, 34, 75, 82, 34, 75, 75, 82, 34, 82, 82, 34, 75, 82, 82, 6, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 82, 11, 52, 52, 82, 82, 11, 82, 20, 61, 61, 82, 11, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 20, 61, 82, 82, 5, 11, 52, 52, 82, 82, 34, 75, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 5, 34, 82, 82, 82, 82, 12, 53, 53, 82, 82, 82, 82, 22, 63, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 11, 52, 82, 20, 82, 82, 82, 6, 82, 82, 23, 82, 82, 82, 82, 82, 6, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 82, 82, 22, 82, 82, 11, 52, 82, 20, 61, 61, 82, 82, 11, 82, 20, 61, 61, 61, 61, 82, 82, 82, 11, 52, 52, 34, 75, 75, 75, 75, 82, 11, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 13, 54, 82, 82, 5, 46, 82, 82, 20, 61, 61, 82, 82, 11, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 82, 82, 11, 52, 52, 52, 52, 82, 82, 20, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 37, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 8, 82, 11, 52, 20, 82, 82, 11, 52, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 82, 11, 52, 82, 20, 61, 61, 61, 61, 82, 82, 34, 75, 82, 11, 52, 52, 82, 82, 20, 61, 82, 82, 82, 82, 82, 82, 12, 82, 34, 75, 75, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 8, 82, 22, 63, 63, 82, 22, 63, 63, 63, 82, 82, 82, 11, 52, 52, 52, 82, 11, 52, 82, 82, 11, 82, 11, 52, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 23, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 82, 82, 22, 82, 22, 63, 63, 1, 82, 82, 82, 82, 82, 37, 78, 78, 78, 78, 78, 82, 37, 78, 82, 82, 82, 82, 8, 82, 82, 82, 22, 63, 63, 63, 82, 11, 52, 82, 20, 82, 82, 82, 82, 82, 11, 82, 82, 5, 46, 82, 82, 82, 82, 82, 12, -100]}, {'labels': [-100, 82, 82, 82, 82, 82, 82, 82, 82, 2, 43, 43, 43, 43, 32, 82, 28, 69, 82, 82, 82, 82, 82, 82, 82, 0, 82, 82, 23, 64, 82, 82, 9, 50, 82, 82, 82, 82, 34, 82, 34, 82, 34, 75, 82, 34, 82, 34, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 15, 56, 56, 82, 82, 82, 82, 23, 64, 82, 9, 50, 50, 50, 50, 82, 82, 82, 82, 22, 82, 82, 82, 82, 82, 82, 6, 82, 82, 23, 64, 9, 50, 50, 82, 82, 82, 82, 20, 61, 61, 61, 61, 82, 34, 75, 75, 82, 34, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 60, 82, 82, 11, 52, 82, 82, 82, 34, 75, 75, 75, 82, 34, 75, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 11, 52, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 11, 82, 5, 46, 82, 5, 82, 82, 11, 52, 82, 20, 61, 61, 61, 82, 20, 61, 61, 61, 82, 20, 61, 61, 82, 82, 11, 82, 11, 82, 20, 61, 61, 61, 61, 82, 20, 61, 61, 61, 61, 82, 82, 82, 11, 52, 82, 5, 82, 34, 75, 75, 75, 75, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 22, 82, 82, 22, 82, 82, 12, 53, 82, 82, 82, 11, 52, 52, 82, 82, 5, 82, 82, 34, 75, 82, 82, 82, 82, 11, 82, 11, 52, 52, 82, 20, 82, 82, 9, 50, 50, 82, 11, 52, 82, 34, 75, 75, 75, 75, 82, 10, 82, 10, 82, 82, 34, 82, 11, 82, 20, 82, 11, 52, 52, 82, 20, 82, 82, 11, 52, 52, 52, 52, 52, 52, 52, 82, 20, 82, 82, 82, 34, 75, 82, 82, 11, 52, 52, 52, 82, 82, 82, 34, 75, 75, 82, 11, 52, 52, 82, 82, 82, 82, 82, 34, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 82, 34, 75, 75, 75, 75, 75, 75, 82, 22, 63, 82, 82, 82, 22, 63, 82, 14, 55, 55, 1, 17, 58, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 34, 75, 82, 82, 5, 46, 82, 82, 82, 11, 52, 82, 20, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 75, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 24, 65, 82, 82, 10, 51, 51, 51, 51, 51, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 9, 50, 50, 50, 50, 50, 50, 50, 82, 82, 9, 50, 50, 82, 82, 11, 52, 52, 52, 52, 82, 82, 34, 75, 75, 75, 75, 75, 75, 75, 75, 82, 20, 61, 61, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 6, 82, 82, 23, 82, 9, 50, 82, 34, 82, 82, 82, 11, 52, 52, 82, 82, 82, 82, 11, 52, 82, 82, 11, 52, 82, 82, 82, 82, 11, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, 10, 51, 51, 82, 82, 82, 82, 82, 82, 22, 82, 22, 63, 82, 82, 37, 78, 78, 82, 82, 82, 82, 82, 82, 82, 82, 9, 50, 50, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 82, 11, 52, 52, 52, 52, 82, 82, 82, 82, 11, 52, 82, 82, 5, 46, 46, 46, 46, 46, 46, 46, 46, 82, 82, 11, 52, 52, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 11, 82, 20, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 82, 82, 11, 52, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 34, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 75, 75, 82, 82, 82, 82, 82, 12, 82, 82, 11, 52, 52, 82, 11, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 46, 46, 46, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 63, 63, 63, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 28, 69, 32, 6, 82, 82, 23, 64, 64, 64, 64, 64, 64, 82, 23, 64, 64, 82, 82, 11, 82, 82, 82, 82, 12, 82, 82, 82, 82, 34, 75, 75, 82, 82, 6, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 0, 82, 82, 82, 82, 82, 82, 82, 0, 41, 82, 82, 0, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 38, 79, 82, 82, 82, 82, 82, 34, 82, 82, 82, 6, 82, 82, 82, 82, 82, 38, 79, 79, 34, 75, 75, 82, 82, 82, 82, 82, 12, 53, 82, 82, 34, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 82, 82, 82, 82, 82, 82, 82, 12, 53, 10, 51, 51, 9, 50, 50, 82, 19, 60, 60, 82, 82, 19, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 53, 9, 50, 50, 82, 82, 82, 82, 23, 64, 82, 82, 82, 11, 52, 82, 82, 82, 82, 11, 20, 61, 61, 61, 61, 82, 11, 52, 20, 61, 61, 61, 61, 82, 11, 52, 20, 61, 61, 61, 82, 11, 52, 20, 61, 61, 61, 61, 82, 82, 82, 11, 52, 82, 20, 61, 82, 10, 51, 82, 82, 11, 52, 82, 82, 82, 82, 34, 82, 34, 82, 10, 82, 10, 82, 82, 10, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 11, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 10, 51, 51, 51, 51, 51, 82, 82, 82, 11, 52, 82, 9, 50, 50, 82, 82, 82, 12, 82, 10, 82, 10, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 22, 82, 22, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 82, 82, 12, 53, 82, 12, 53, 9, 50, 50, 82, 82, 82, 82, 82, 82, 12, 53, 53, 82, 9, 50, 50, 50, 82, 82, 34, 82, 34, 75, 75, 82, 5, 46, 46, 46, 82, 82, 82, 12, 53, 53, 53, 82, 82, 82, 6, 82, 82, 82, 82, 82, 82, 82, 82, 13, 54, 54, 82, 34, 82, 5, 46, 46, 46, 46, 46, 46, 46, 82, 10, 51, 51, 51, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 11, 82, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 20, 61, 61, 11, 52, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 61, 82, 11, 52, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 61, 61, 82, 11, 52, 52, 52, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 61, 82, 11, 52, 52, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 61, 61, 61, 82, 82, 10, 51, 51, 11, 52, 82, 11, 82, 82, 82, 10, 82, 10, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 11, 52, 52, 52, 82, 82, 34, 82, 82, 34, 82, 82, 82, 15, 56, 56, 82, 82, 82, 22, 63, 82, 22, 63, 63, 82, 14, 55, 55, 55, 82, 82, 82, 82, 11, 82, 82, 20, 61, 61, 61, 61, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 31, 82, 82, 82, 34, 82, 5, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 82, 82, 82, 37, 78, 78, 78, 78, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 34, 82, 5, 46, 82, 37, 78, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 11, 82, 20, 82, 82, 20, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 82, 82, 82, 14, 55, 55, 55, 55, 55, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 34, 9, 50, 50, 82, 82, 11, 52, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 82, 10, 51, 82, 10, 51, 51, 82, 82, 82, 10, 51, 51, 51, 51, 51, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 19, 60, 60, 60, 60, 60, 82, 82, 82, 82, 34, 82, 3, 44, 44, 44, 44, 44, 44, 44, 82, 82, 5, 46, 46, 46, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 9, 50, 82, 82, 23, 64, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 60, 60, 60, 82, 11, 52, 82, 82, 82, 34, 82, 11, 52, 82, 20, 61, 61, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 82, 11, 82, 82, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 11, 52, 52, 82, 20, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 82, 37, 78, 78, 82, 82, 11, 52, 82, 82, 12, 82, 12, 82, 82, 82, 82, 82, 82, 82, 11, 82, 11, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 52, 52, 52, 82, 11, 82, 82, 11, 52, 52, 82, 82, 82, 11, 52, 52, 52, 82, 11, 82, 11, 82, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 20, 61, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 8, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 82, 82, 82, 82, 37, 82, 82, 82, 6, 17, 58, 58, 82, 82, 82, 82, 82, 11, 82, 82, 20, 82, 82, 82, 82, 82, 82, 34, 82, 9, 50, 82, 82, 82, 82, 6, 82, 82, 23, 82, 82, 23, 64, 64, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 34, 75, 82, 82, 5, 46, 46, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 52, 52, 52, 82, 11, 52, 52, 52, 52, 52, 82, 82, 82, 20, 61, 61, 61, 61, 61, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 11, 82, 11, 82, 82, 82, 82, 34, 82, 5, 46, 46, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 20, 61, 61, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 11, 52, 82, 82, 82, 82, 11, 82, 11, 82, 11, 52, 82, 82, 11, 52, 52, 52, 82, 82, 11, 52, 52, 52, 82, 11, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 53, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 34, 75, 75, 75, 75, 82, 82, 82, 82, 82, 34, 75, 75, 75, 82, 10, 51, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 82, 82, 82, 0, 41, 6, 82, 23, 82, 82, 82, 82, 82, 82, 82, 34, 82, 11, 52, 52, 82, 82, 19, 60, 60, 60, 60, 60, 60, 82, 82, 82, 82, 82, 22, 82, 22, 63, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 46, 82, 82, 82, 82, 13, 54, 54, 54, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 46, 82, 5, 46, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 5, 46, 46, 82, 12, 53, 82, 82, 10, 51, 82, 82, 82, 82, 34, 82, 82, 13, 54, 54, 54, 54, 54, 54, 54, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 82, 10, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 11, 52, 82, 82, 82, 82, 11, 52, 52, 52, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 12, 53, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 22, 82, 14, 55, 55, 55, 55, 55, 55, 82, 9, 50, 82, 82, 22, 82, 14, 55, 55, 55, 55, 55, 55, 82, 9, 50, 50, 50, 82, 82, 82, 9, 50, 50, 50, 50, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 9, 50, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 11, 52, 82, 34, 82, 20, 61, 61, 61, 82, 82, 34, 82, 20, 61, 61, 61, 82, 82, 82, 11, 52, 82, 82, 34, 82, 11, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 20, 82, 82, 82, 82, 82, 82, 82, 15, 56, 56, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 34, 82, 82, 82, 34, 82, 82, 5, 46, 82, 82, 82, 82, 34, 75, 75, 75, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 34, 82, 10, 51, 51, 51, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 34, 82, 82, 82, 11, 82, 82, 5, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 12, 53, 53, 53, 82, 11, 52, 82, 82, 39, 80, 80, 80, 17, 58, 58, 82, 9, 82, 9, 50, 50, 82, 82, 82, 82, 34, 82, 34, 82, 34, 82, 34, 82, 82, 82, 82, 82, 34, 82, 11, 52, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 34, 82, 82, 5, 82, 82, 11, 52, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 11, 52, 52, 52, 52, 52, 52, 52, 82, 82, 82, 82, 82, 34, 75, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 34, 5, 46, 82, 82, 82, 82, 82, 34, 82, 3, 44, 44, 44, 44, 44, 44, 44, 82, 82, 82, 82, 5, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 23, 64, 82, 82, 82, 82, 82, 22, 63, 63, 63, 63, 14, 55, 55, 55, 55, 55, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 82, 22, 82, 82, 82, 82, 22, 82, 82, 82, 82, 22, 63, 63, 63, 82, 82, 82, 82, 82, 22, 82, 14, 55, 82, 82, 9, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 9, 50, 50, 82, 82, 82, 82, 82, 12, 53, 82, 82, 82, 34, 75, 82, 34, 82, 82, 34, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 9, 50, 82, 82, 23, 82, 82, 82, 35, 82, 82, 82, 82, 82, 82, 82, 82, 82, 23, 82, 11, 52, 82, 37, 82, 82, 82, 82, 82, 82, 12, 82, 82, 11, 82, 11, 52, 82, 11, 82, 82, 82, 82, 12, 53, 82, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 22, 82, 22, 63, 63, 63, 63, 63, 82, 82, 82, 82, 82, 34, 82, 82, 11, 52, 82, 82, 82, 82, 34, 82, 12, 53, 82, 9, 50, 50, 50, 50, 6, 47, 47, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 6, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 15, 56, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 12, 53, 82, 11, 52, 52, 82, 11, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 34, 82, 82, 5, 82, 82, 82, 82, 12, 82, 82, 34, 82, 82, 5, 46, 46, 82, 82, 8, 82, 82, 82, 82, 34, 75, 75, 75, 82, 82, 11, 52, 52, 52, 82, 11, 52, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 11, 52, 52, 52, 82, 11, 52, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 5, 46, 46, 82, 82, 82, 8, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 78, 78, 78, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 82, 34, 75, 82, 82, 34, 82, 15, 56, 82, 82, 11, 52, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 82, 19, 60, 60, 60, 60, 82, 11, 52, 52, 52, 52, 52, 52, 82, 11, 82, 82, 82, 82, 82, 34, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 5, 46, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 5, 82, 82, 5, 46, 46, 46, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 5, 82, 82, 10, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 3, 44, 44, 44, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 6, 82, 9, 50, 50, 82, 34, 82, 82, 34, 20, 82, 82, 82, 82, 82, 22, 63, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 6, 82, 82, 23, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 15, 56, 56, 56, 82, 82, 34, 75, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 12, 82, 82, 6, 82, 23, 64, 82, 82, 82, 82, 82, 6, 47, 82, 82, 23, 82, 82, 82, 82, 34, 82, 34, 82, 82, 82, 82, 11, 52, 52, 52, 52, 82, 82, 82, 82, 82, 12, 53, 53, 82, 12, 82, 82, 82, 11, 82, 82, 82, 34, 75, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 82, 82, 12, 53, 82, 82, 82, 34, 75, 75, 75, 82, 82, 12, 53, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 11, 52, 52, 52, 52, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 11, 82, 20, 61, 61, 61, 61, 82, 82, 34, 82, 11, 52, 82, 20, 61, 61, 61, 61, 61, 61, 82, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 52, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 20, 61, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 11, 52, 52, 52, 52, 52, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 11, 82, 11, 82, 82, 10, 82, 20, 82, 82, 11, 52, 52, 52, 82, 82, 82, 11, 52, 52, 82, 82, 11, 52, 52, 82, 82, 11, 52, 52, 52, 52, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 13, 54, 82, 82, 82, 82, 34, 75, 75, 75, 82, 12, 53, 82, 82, 11, 52, 82, 20, 61, 82, 82, 82, 82, 82, 82, 34, 82, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 2, 43, 43, 43, 43, 32, 82, 6, 82, 82, 23, 64, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 5, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 10, 82, 10, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 82, 19, 60, 60, 60, 60, 60, 82, 82, 82, 82, 11, 82, 34, 82, 82, 82, 5, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 75, 82, 10, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 10, 51, 51, 51, 51, 82, 82, 34, 82, 82, 5, 46, 46, 46, 46, 46, 46, 82, 82, 34, 82, 82, 82, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 5, 46, 46, 46, 46, 46, 46, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 82, 6, 47, 82, 10, 51, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 32, 19, 60, 60, 60, 60, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 12, 82, 37, 78, 78, 82, 10, 51, 82, 82, 82, 82, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 11, 52, 52, 82, 11, 82, 82, 82, 82, 82, 82, 37, 78, 78, 78, 78, 78, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 49, 49, 49, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8, 82, 82, 82, 82, 82, 10, 51, 51, 51, 51, 51, 82, 82, 5, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 22, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 5, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 53, 53, 82, 82, 82, 82, 82, 8, 82, 82, 10, 51, 51, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 37, 82, 82, 82, 82, 82, 82, 82, 82, 6, 82, 82, 23, 64, 82, 11, 52, 82, 82, 10, 51, 51, 8, 82, 82, 82, 82, 12, 82, 5, 46, 46, 46, 46, 82, 82, 8, 82, 37, 82, 82, 82, 82, 82, 82, 8, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 10, 51, 51, 82, 82, 82, 37, 82, 82, 82, 82, 11, 20, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 37, 78, 82, 82, 37, 82, 82, 9, 50, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 11, 52, 20, 61, 82, 82, 82, 82, 82, 82, 82, 82, 37, 82, 82, 9, 50, 82, 82, 6, 23, 82, 82, 9, 50, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 82, 9, 50, 82, 6, 82, 82, 23, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 23, 64, 64, 64, 64, 64, 64, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 15, 56, 82, 82, 82, 82, 82, 34, 75, 75, 75, 82, 82, 82, 15, 56, 82, 34, 75, 75, 75, 75, 82, 82, 34, 75, 75, 75, 82, 11, 82, 11, 52, 52, 82, 20, 61, 61, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 8, 82, 10, 51, 51, 82, 11, 82, 82, 82, 82, 82, 82, 82, 82, 11, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 5, 46, 46, 46, 46, 46, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 23, 64, 82, 82, 11, 52, 82, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 52, 82, 20, 61, 61, 82, 82, 82, 82, 82, 12, 53, 53, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 14, 55, 55, 55, 55, 55, 55, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 32, 82, 82, 2, 43, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 27, 9, 50, 50, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82, 82, 35, 76, 76, 76, 82, 23, 82, 82, 12, 53, 82, 82, 82, 82, 11, 52, 52, 52, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 11, 52, 82, 82, 82, 82, 82, 82, 12, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 61, 61, 61, 61, 82, 82, 5, 46, 82, 5, 82, 82, 82, 82, 82, 12, 53, 82, 82, 82, 82, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 82, 12, 53, 82, 20, 61, 61, 61, 82, 19, 82, 12, 53, 53, 82, 82, 12, 53, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 23, 82, 82, 82, 11, 52, 52, 52, 82, 82, 11, 52, 52, 52, 52, 52, 82, 20, 61, 61, 82, 20, 61, 82, 82, 82, 20, 61, 61, 82, 20, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 35, 76, 76, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 11, 52, 82, 2, 43, 43, 43, 82, 82, 82, 11, 52, 52, 52, 52, 82, 82, 82, 11, 52, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 61, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 82, 82, 11, 52, 82, 82, 9, 50, 50, 10, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 82, 11, 52, 52, 52, 82, 20, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 82, 82, 82, 82, 5, 82, 82, 20, 61, 61, 82, 82, 82, 82, 82, 82, 82, 82, 82, 20, 61, 82, 82, 5, 82, 82, 82, 82, 82, 82, 11, 52, 82, 82, 82, 11, 82, 82, 82, 82, 82, 5, 82, 5, 82, 82, 82, 82, 82, 82, 82, 12, 82, 11, 52, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 82, 11, 52, 82, 11, 52, 82, 11, 52, 82, 11, 82, 82, 82, 12, 82, 82, 82, 5, 46, 82, 82, 39, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 82, 82, 82, 82, 82, 82, 82, 82, 82, 5, 46, 46, 46, 46, 46, 82, 5, 46, 46, 46, 46, 82, 82, 5, 46, 46, 82, 82, 5, 46, 82, 10, 51, 51, 82, 10, 51, 51, 51, 82, 82, 12, 82, 82, 11, 82, 82, 11, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 0, 82, 82, 15, 56, 56, 56, 82, 34, 82, 10, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 32, 82, 34, 75, 82, 34, 82, 34, 82, 82, 82, 34, 82, 6, 82, 82, 82, 11, 52, 82, 82, 82, 12, 53, 53, 53, 82, 82, 82, 11, 82, 82, 5, 82, 82, 82, 82, 23, 82, 82, 82, 10, 51, 51, 51, 82, 82, 82, 82, 11, 52, 52, 82, 82, 22, 63, 63, 14, 55, 55, 55, 55, 55, 55, 82, 10, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 22, 63, 63, 82, 14, 55, 55, 55, 55, 82, 10, 51, 51, 51, 51, 82, 82, 82, 8, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 10, 51, 51, 51, 51, 51, 51, 51, 51, 51, 82, 10, 51, 51, 51, 51, 51, 51, 51, 51, 51, 82, 82, 11, 52, 82, 82, 5, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 82, 82, 82, 5, 46, 46, 82, 10, 51, 51, 51, 51, 82, 82, 82, 82, 82, 82, 82, 82, 82, 19, 60, 60, 60, 60, 60, 60, 60, 60, 60, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 37, 78, 82, 82, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 53, 82, -100]}, {'labels': [-100, 82, 2, 43, 43, 43, 43, 28, 32, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, -100]}, {'labels': [-100, 82, 82, 2, 43, 43, 82, 19, 60, 82, 19, 60, 60, 60, 60, 82, 82, 82, 37, 78, 82, 82, 82, 82, 11, 52, 82, 11, 52, 82, 82, 6, 47, 47, 82, 20, 82, 82, 82, 82, 37, 78, 78, 82, 82, 82, 82, 34, 75, 82, 82, 82, 82, 82, 82, 37, 82, 82, 82, 82, 82, 82, 34, 82, 20, 61, 61, 61, 61, 82, 82, 20, 38, 79, 79, 79, 82, 37, 82, 82, 6, 47, 47, 47, 82, 38, 79, 79, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 82, 34, 75, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 82, 82, 82, 34, 75, 82, 82, 82, 38, 79, 79, 82, 82, 82, 37, 82, 82, 82, 22, 63, 82, 82, 82, 82, 82, 11, 52, 82, 82, 34, 75, 75, 75, 82, 82, 11, 52, 82, 20, 61, 61, 61, 82, 82, 11, 52, 52, 82, 20, 82, 82, 34, 75, 82, 82, 82, 82, 82, 34, 82, 34, 75, 82, 82, 82, 82, 11, 52, 52, 52, 82, 10, 51, 82, 20, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 34, 75, 75, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 12, 53, 53, 82, 82, 11, 52, 82, 11, 52, 82, 82, 82, 82, 82, 11, 52, 20, 61, 61, 61, 82, 82, 34, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 6, 82, 82, 11, 82, 9, 50, 82, 82, 22, 63, 63, 82, 22, 63, 63, 82, 82, 15, 56, 82, 82, 11, 82, 82, 82, 82, 82, 9, 50, 50, 82, 82, 82, 82, 82, 34, 82, 82, 82, -100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb seqeval --quiet\n",
    "!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:48:30.856099Z",
     "iopub.status.busy": "2025-11-13T18:48:30.855465Z",
     "iopub.status.idle": "2025-11-13T18:48:30.862344Z",
     "shell.execute_reply": "2025-11-13T18:48:30.861712Z",
     "shell.execute_reply.started": "2025-11-13T18:48:30.856076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:48:31.118416Z",
     "iopub.status.busy": "2025-11-13T18:48:31.118159Z",
     "iopub.status.idle": "2025-11-13T18:48:40.183477Z",
     "shell.execute_reply": "2025-11-13T18:48:40.182906Z",
     "shell.execute_reply.started": "2025-11-13T18:48:31.118397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkshitij-nevrekar\u001b[0m (\u001b[33mkshitij-nevrekar-nmims\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "\n",
    "# Load your secret\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login\n",
    "wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:48:40.185461Z",
     "iopub.status.busy": "2025-11-13T18:48:40.184759Z",
     "iopub.status.idle": "2025-11-13T18:48:58.314124Z",
     "shell.execute_reply": "2025-11-13T18:48:58.313304Z",
     "shell.execute_reply.started": "2025-11-13T18:48:40.185442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:48:45.809997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763059725.832333     329 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763059725.839141     329 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251113_184851-kgvjff8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/kgvjff8z' target=\"_blank\">pubmedbert-maccrobat-v1</a></strong> to <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner' target=\"_blank\">https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/kgvjff8z' target=\"_blank\">https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/kgvjff8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W&B initialized\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"biomedical-ner\",\n",
    "    name=\"pubmedbert-maccrobat-v1\",\n",
    "    tags=[\"pubmedbert\", \"maccrobat\", \"baseline\"]\n",
    ")\n",
    "\n",
    "print(\" W&B initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:48:58.315240Z",
     "iopub.status.busy": "2025-11-13T18:48:58.314985Z",
     "iopub.status.idle": "2025-11-13T18:49:01.275058Z",
     "shell.execute_reply": "2025-11-13T18:49:01.274246Z",
     "shell.execute_reply.started": "2025-11-13T18:48:58.315211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 400\n",
      "Example structure:\n",
      "  Keys: dict_keys(['tokens', 'tags'])\n",
      "  Tokens: ['A', '68', '-', 'year', '-', 'old', 'female', 'nonsmoker', ',', 'nondrinker']\n",
      "  Tags: ['O', 'B-Age', 'I-Age', 'I-Age', 'I-Age', 'I-Age', 'B-Sex', 'B-History', 'O', 'B-History']\n",
      "\n",
      " Found 83 unique entity types\n",
      "Entity types: ['B-Activity', 'B-Administration', 'B-Age', 'B-Area', 'B-Biological_attribute', 'B-Biological_structure', 'B-Clinical_event', 'B-Color', 'B-Coreference', 'B-Date']...\n",
      "Sample tags: B-Age=2, O=82\n"
     ]
    }
   ],
   "source": [
    "# Load MACCROBAT from HuggingFace\n",
    "dataset = load_dataset(\"ktgiahieu/maccrobat2018_2020\")\n",
    "\n",
    "# Check structure\n",
    "print(f\"Train examples: {len(dataset['train'])}\")\n",
    "print(f\"Example structure:\")\n",
    "example = dataset['train'][0]\n",
    "print(f\"  Keys: {example.keys()}\")\n",
    "print(f\"  Tokens: {example['tokens'][:10]}\")\n",
    "print(f\"  Tags: {example['tags'][:10]}\")\n",
    "\n",
    "# Get unique tags for num_labels\n",
    "unique_tags = set()\n",
    "for example in dataset['train']:\n",
    "    unique_tags.update(example['tags'])\n",
    "\n",
    "# Ensure every B-XXX tag has a corresponding I-XXX tag\n",
    "extra_i_tags = set()\n",
    "for tag in unique_tags:\n",
    "    if tag.startswith(\"B-\"):\n",
    "        i_tag = \"I-\" + tag[2:]\n",
    "        if i_tag not in unique_tags:\n",
    "            extra_i_tags.add(i_tag)\n",
    "unique_tags.update(extra_i_tags)\n",
    "\n",
    "unique_tags = sorted(list(unique_tags))\n",
    "num_labels = len(unique_tags)\n",
    "tag2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "print(f\"\\n Found {num_labels} unique entity types\")\n",
    "print(f\"Entity types: {unique_tags[:10]}...\")\n",
    "print(f\"Sample tags: B-Age={tag2id.get('B-Age')}, O={tag2id.get('O')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.277091Z",
     "iopub.status.busy": "2025-11-13T18:49:01.276811Z",
     "iopub.status.idle": "2025-11-13T18:49:01.286973Z",
     "shell.execute_reply": "2025-11-13T18:49:01.286314Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.277074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DEBUG: Checking raw dataset structure...\n",
      "Sample keys: dict_keys(['tokens', 'tags'])\n",
      "Sample tokens type: <class 'list'>\n",
      "Sample tokens length: 393\n",
      "First 10 tokens: ['A', '68', '-', 'year', '-', 'old', 'female', 'nonsmoker', ',', 'nondrinker']\n",
      "First 10 tags: ['O', 'B-Age', 'I-Age', 'I-Age', 'I-Age', 'I-Age', 'B-Sex', 'B-History', 'O', 'B-History']\n"
     ]
    }
   ],
   "source": [
    "# Debug - Check raw data before tokenization\n",
    "print(\"\\n DEBUG: Checking raw dataset structure...\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Sample tokens type: {type(sample['tokens'])}\")\n",
    "print(f\"Sample tokens length: {len(sample['tokens'])}\")\n",
    "print(f\"First 10 tokens: {sample['tokens'][:10]}\")\n",
    "print(f\"First 10 tags: {sample['tags'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.287809Z",
     "iopub.status.busy": "2025-11-13T18:49:01.287633Z",
     "iopub.status.idle": "2025-11-13T18:49:01.775298Z",
     "shell.execute_reply": "2025-11-13T18:49:01.774727Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.287788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text and align labels with tokens.\n",
    "    \n",
    "    Key changes:\n",
    "    - Added validation for empty examples  #  NEW: Data validation\n",
    "    - Better error handling for missing tags  #  NEW: Error handling\n",
    "    - Filter out problematic examples  #  NEW: Filtering\n",
    "    \"\"\"\n",
    "    #  Validate input - filter out empty examples\n",
    "    valid_indices = []\n",
    "    valid_tokens = []\n",
    "    valid_tags = []\n",
    "    \n",
    "    for i, (tokens, tags) in enumerate(zip(examples[\"tokens\"], examples[\"tags\"])):\n",
    "        #  NEW: Check if example is valid\n",
    "        if tokens and tags and len(tokens) == len(tags):\n",
    "            valid_indices.append(i)\n",
    "            valid_tokens.append(tokens)\n",
    "            valid_tags.append(tags)\n",
    "        else:\n",
    "            print(f\"  Skipping invalid example {i}: tokens={len(tokens) if tokens else 0}, tags={len(tags) if tags else 0}\")\n",
    "    \n",
    "    #  NEW: If no valid examples, return empty dict\n",
    "    if not valid_tokens:\n",
    "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    \n",
    "    # Tokenize valid examples only\n",
    "    tokenized_inputs = tokenizer(\n",
    "        valid_tokens,                    #  CHANGED: Use filtered tokens\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    #  Iterate over valid examples\n",
    "    for i, label in enumerate(valid_tags):  #  CHANGED: Use filtered tags\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                #  NEW: Add bounds checking\n",
    "                if word_idx < len(label):\n",
    "                    label_ids.append(tag2id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)  # Safety fallback\n",
    "            else:\n",
    "                #  NEW: Add bounds checking\n",
    "                if word_idx < len(label):\n",
    "                    original_tag = label[word_idx]\n",
    "                    if original_tag.startswith(\"B-\"):\n",
    "                        i_tag = \"I-\" + original_tag[2:]\n",
    "                        if i_tag in tag2id:\n",
    "                            label_ids.append(tag2id[i_tag])\n",
    "                        else:\n",
    "                            label_ids.append(tag2id[original_tag])\n",
    "                    elif original_tag == \"O\":\n",
    "                        label_ids.append(tag2id[\"O\"])\n",
    "                    else:\n",
    "                        label_ids.append(tag2id[original_tag])\n",
    "                else:\n",
    "                    label_ids.append(-100)  # Safety fallback\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.776209Z",
     "iopub.status.busy": "2025-11-13T18:49:01.776000Z",
     "iopub.status.idle": "2025-11-13T18:49:01.798413Z",
     "shell.execute_reply": "2025-11-13T18:49:01.797853Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.776192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing tokenization on single example...\n",
      "Input tokens (first 5): ['A', '68', '-', 'year', '-']\n",
      "Input tags (first 5): ['O', 'B-Age', 'I-Age', 'I-Age', 'I-Age']\n",
      "\n",
      " Tokenization test results:\n",
      "   Number of examples processed: 1\n",
      "   First example input_ids length: 455\n",
      "   First example labels length: 455\n",
      "   First 10 input_ids: [2, 43, 5117, 17, 2476, 17, 4156, 4232, 26523, 1915]\n",
      "   First 10 labels: [-100, 82, 2, 43, 43, 43, 43, 32, 19, 60]\n",
      "   First 10 decoded tokens: ['[CLS]', 'a', '68', '-', 'year', '-', 'old', 'female', 'nonsmok', '##er']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Testing tokenization on single example...\")\n",
    "test_example = dataset['train'][0]\n",
    "print(f\"Input tokens (first 5): {test_example['tokens'][:5]}\")\n",
    "print(f\"Input tags (first 5): {test_example['tags'][:5]}\")\n",
    "\n",
    "#  Must wrap in lists for batched processing (this is how .map() calls it)\n",
    "test_result = tokenize_and_align_labels({\n",
    "    'tokens': [test_example['tokens']],    #  List of token lists\n",
    "    'tags': [test_example['tags']]         #  List of tag lists\n",
    "})\n",
    "\n",
    "print(f\"\\n Tokenization test results:\")\n",
    "print(f\"   Number of examples processed: {len(test_result['input_ids'])}\")\n",
    "print(f\"   First example input_ids length: {len(test_result['input_ids'][0])}\")\n",
    "print(f\"   First example labels length: {len(test_result['labels'][0])}\")\n",
    "print(f\"   First 10 input_ids: {test_result['input_ids'][0][:10]}\")\n",
    "print(f\"   First 10 labels: {test_result['labels'][0][:10]}\")\n",
    "print(f\"   First 10 decoded tokens: {tokenizer.convert_ids_to_tokens(test_result['input_ids'][0][:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.799209Z",
     "iopub.status.busy": "2025-11-13T18:49:01.798942Z",
     "iopub.status.idle": "2025-11-13T18:49:01.804252Z",
     "shell.execute_reply": "2025-11-13T18:49:01.803357Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.799184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Apply tokenization to dataset\n",
    "# print(\"Tokenizing dataset...\")\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_and_align_labels,\n",
    "#     batched=True,\n",
    "#     remove_columns=dataset[\"train\"].column_names\n",
    "# )\n",
    "\n",
    "# print(f\" Tokenized train examples: {len(tokenized_dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.805350Z",
     "iopub.status.busy": "2025-11-13T18:49:01.805183Z",
     "iopub.status.idle": "2025-11-13T18:49:01.824012Z",
     "shell.execute_reply": "2025-11-13T18:49:01.823294Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.805337Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data collator created\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Data collator pads sequences to same length in each batch\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "print(\" Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.825112Z",
     "iopub.status.busy": "2025-11-13T18:49:01.824798Z",
     "iopub.status.idle": "2025-11-13T18:49:01.836483Z",
     "shell.execute_reply": "2025-11-13T18:49:01.835911Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.825089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute standard NER metrics: precision, recall, F1.\n",
    "    Uses seqeval for entity-level evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove -100 labels (special tokens we ignored)\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    #  Calculate CORRECT metrics using seqeval\n",
    "    results = {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:01.838962Z",
     "iopub.status.busy": "2025-11-13T18:49:01.838447Z",
     "iopub.status.idle": "2025-11-13T18:49:02.248218Z",
     "shell.execute_reply": "2025-11-13T18:49:02.247504Z",
     "shell.execute_reply.started": "2025-11-13T18:49:01.838945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class weights calculated\n",
      "   Rare entity weight example: 2121.55x\n",
      "   Common entity weight example: 1.00x\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_class_weights(dataset, tag2id):\n",
    "    \"\"\"Calculate inverse frequency weights for class imbalance\"\"\"\n",
    "    tag_counts = Counter()\n",
    "    \n",
    "    for example in dataset['train']:\n",
    "        for tag in example['tags']:\n",
    "            tag_counts[tag] += 1\n",
    "    \n",
    "    # Calculate weights (inverse frequency)\n",
    "    total = sum(tag_counts.values())\n",
    "    weights = {}\n",
    "    for tag, count in tag_counts.items():\n",
    "        # Inverse frequency with smoothing\n",
    "        weights[tag2id[tag]] = total / (count + 100)  # +100 for smoothing\n",
    "    \n",
    "    # Convert to tensor\n",
    "    weight_list = [weights.get(i, 1.0) for i in range(len(tag2id))]\n",
    "    class_weights = torch.tensor(weight_list, dtype=torch.float32)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "class_weights = calculate_class_weights(dataset, tag2id)\n",
    "print(f\"\\n Class weights calculated\")\n",
    "print(f\"   Rare entity weight example: {class_weights.max():.2f}x\")\n",
    "print(f\"   Common entity weight example: {class_weights.min():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:02.249126Z",
     "iopub.status.busy": "2025-11-13T18:49:02.248854Z",
     "iopub.status.idle": "2025-11-13T18:49:04.178554Z",
     "shell.execute_reply": "2025-11-13T18:49:04.177840Z",
     "shell.execute_reply.started": "2025-11-13T18:49:02.249108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
      "   Parameters: 108,955,475\n",
      "   Entity types: 83\n",
      "\n",
      " Label mappings set:\n",
      "   Sample id2label: B-Activity, B-Administration, B-Age\n",
      "   Sample label2id: B-Age=2, O=82\n"
     ]
    }
   ],
   "source": [
    "# Load PubMedBERT with token classification head\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id\n",
    ")\n",
    "\n",
    "# Move weights to same device as model\n",
    "if torch.cuda.is_available():\n",
    "    class_weights = class_weights.cuda()\n",
    "\n",
    "# Monkey-patch the model's loss to use weighted cross entropy\n",
    "original_forward = model.forward\n",
    "\n",
    "def weighted_forward(*args, **kwargs):\n",
    "    outputs = original_forward(*args, **kwargs)\n",
    "    \n",
    "    # If loss is computed, recompute with weights\n",
    "    if outputs.loss is not None and 'labels' in kwargs:\n",
    "        from torch.nn import CrossEntropyLoss\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        labels = kwargs['labels']\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        outputs.loss = loss\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "model.forward = weighted_forward\n",
    "\n",
    "print(f\" Model loaded: {model_name}\")\n",
    "print(f\"   Parameters: {model.num_parameters():,}\")\n",
    "print(f\"   Entity types: {num_labels}\")\n",
    "\n",
    "print(f\"\\n Label mappings set:\")\n",
    "print(f\"   Sample id2label: {id2tag[0]}, {id2tag[1]}, {id2tag[2]}\")\n",
    "print(f\"   Sample label2id: B-Age={tag2id.get('B-Age')}, O={tag2id.get('O')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:04.179464Z",
     "iopub.status.busy": "2025-11-13T18:49:04.179288Z",
     "iopub.status.idle": "2025-11-13T18:49:04.215397Z",
     "shell.execute_reply": "2025-11-13T18:49:04.214843Z",
     "shell.execute_reply.started": "2025-11-13T18:49:04.179450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pubmedbert-maccrobat\",\n",
    "\n",
    "    # Data\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "\n",
    "    # Epochs & steps\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Logging & saving\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "\n",
    "    # W&B integration\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"pubmedbert-maccrobat-v2\",\n",
    "\n",
    "    # Misc\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "print(\" Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:04.216264Z",
     "iopub.status.busy": "2025-11-13T18:49:04.216072Z",
     "iopub.status.idle": "2025-11-13T18:49:05.380073Z",
     "shell.execute_reply": "2025-11-13T18:49:05.379500Z",
     "shell.execute_reply.started": "2025-11-13T18:49:04.216247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting dataset into train/validation...\n",
      "Raw train size: 360\n",
      "Raw validation size: 40\n",
      "\n",
      "Tokenizing both splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff38bde304ac4075b2f8e3426c14291d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993bec90f17642bbb4a5a1efd00796d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenized train size: 360\n",
      " Tokenized validation size: 40\n"
     ]
    }
   ],
   "source": [
    "# Create split BEFORE tokenization\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Split BEFORE tokenization (on raw data)\n",
    "print(\"\\nSplitting dataset into train/validation...\")\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": split_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "print(f\"Raw train size: {len(dataset['train'])}\")\n",
    "print(f\"Raw validation size: {len(dataset['validation'])}\")\n",
    "\n",
    "# Then tokenize BOTH splits\n",
    "print(\"\\nTokenizing both splits...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\" Tokenized train size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\" Tokenized validation size: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:05.381103Z",
     "iopub.status.busy": "2025-11-13T18:49:05.380921Z",
     "iopub.status.idle": "2025-11-13T18:49:05.518341Z",
     "shell.execute_reply": "2025-11-13T18:49:05.517777Z",
     "shell.execute_reply.started": "2025-11-13T18:49:05.381089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\" Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:49:05.519557Z",
     "iopub.status.busy": "2025-11-13T18:49:05.519098Z",
     "iopub.status.idle": "2025-11-13T18:49:05.533071Z",
     "shell.execute_reply": "2025-11-13T18:49:05.532344Z",
     "shell.execute_reply.started": "2025-11-13T18:49:05.519538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DIAGNOSTIC: Checking tokenized dataset...\n",
      "================================================================================\n",
      "Train dataset size: 360\n",
      "Validation dataset size: 40\n",
      "\n",
      " First training example:\n",
      "   Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "   input_ids length: 512\n",
      "   labels length: 512\n",
      "   First 10 input_ids: [2, 43, 3259, 17, 2476, 17, 4156, 10440, 1982, 9723]\n",
      "   First 10 labels: [-100, 82, 2, 43, 43, 43, 43, 32, 82, 82]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#  NEW: Add this entire diagnostic section\n",
    "print(\"\\n DIAGNOSTIC: Checking tokenized dataset...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if datasets are empty\n",
    "print(f\"Train dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_dataset['validation'])}\")\n",
    "\n",
    "if len(tokenized_dataset['train']) == 0:\n",
    "    print(\" ERROR: Training dataset is empty after tokenization!\")\n",
    "    print(\"   This means all examples were filtered out.\")\n",
    "    print(\"   Check the warnings above for details.\")\n",
    "else:\n",
    "    # Check first example structure\n",
    "    first_example = tokenized_dataset['train'][0]\n",
    "    print(f\"\\n First training example:\")\n",
    "    print(f\"   Keys: {first_example.keys()}\")\n",
    "    print(f\"   input_ids length: {len(first_example['input_ids'])}\")\n",
    "    print(f\"   labels length: {len(first_example['labels'])}\")\n",
    "    print(f\"   First 10 input_ids: {first_example['input_ids'][:10]}\")\n",
    "    print(f\"   First 10 labels: {first_example['labels'][:10]}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    if len(first_example['input_ids']) == 0:\n",
    "        print(\"    ERROR: input_ids is empty!\")\n",
    "    if len(first_example['labels']) == 0:\n",
    "        print(\"    ERROR: labels is empty!\")\n",
    "    if len(first_example['input_ids']) != len(first_example['labels']):\n",
    "        print(f\"     WARNING: Mismatch - input_ids({len(first_example['input_ids'])}) != labels({len(first_example['labels'])})\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T18:51:33.275263Z",
     "iopub.status.busy": "2025-11-13T18:51:33.274517Z",
     "iopub.status.idle": "2025-11-13T18:51:34.006930Z",
     "shell.execute_reply": "2025-11-13T18:51:34.005725Z",
     "shell.execute_reply.started": "2025-11-13T18:51:33.275232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Device: Tesla T4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_329/3183201256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2500\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2502\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2503\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2504\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5299\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5300\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5301\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5302\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"np\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mno_labels_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         batch = pad_without_fast_tokenizer_warning(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mno_labels_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpad_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Restore the state of the warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3289\u001b[0m         \u001b[0;31m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3292\u001b[0m                 \u001b[0;34m\"You should supply an encoding or a list of encodings to this method \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m                 \u001b[0;34mf\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n Training complete!\")\n",
    "print(f\"Final loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Log training summary to W&B\n",
    "wandb.log({\n",
    "    \"train/final_loss\": train_result.training_loss,\n",
    "    \"train/total_epochs\": training_args.num_train_epochs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced evaluation with W&B logging\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Log comprehensive evaluation metrics to W&B\n",
    "wandb.log({\n",
    "    \"eval/final_loss\": eval_results[\"eval_loss\"],\n",
    "    \"eval/final_f1\": eval_results[\"eval_f1\"],\n",
    "    \"eval/final_precision\": eval_results[\"eval_precision\"],\n",
    "    \"eval/final_recall\": eval_results[\"eval_recall\"],\n",
    "})\n",
    "\n",
    "# Save final model locally first (needed for W&B upload)\n",
    "local_model_path = \"./pubmedbert-maccrobat-final\"\n",
    "trainer.save_model(local_model_path)\n",
    "\n",
    "# Save label mappings to config\n",
    "import json\n",
    "config_path = f\"{local_model_path}/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['id2label'] = id2tag\n",
    "config['label2id'] = tag2id\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n Model saved locally to {local_model_path}\")\n",
    "print(f\" Label mappings saved to config.json ({len(id2tag)} labels)\")\n",
    "\n",
    "# Save model to W&B\n",
    "print(\"\\n Uploading model to W&B...\")\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"pubmedbert-maccrobat-ner\",\n",
    "    type=\"model\",\n",
    "    description=f\"PubMedBERT fine-tuned on MACCROBAT NER with {len(id2tag)} entity types\",\n",
    "    metadata={\n",
    "        \"num_labels\": len(id2tag),\n",
    "        \"model_name\": \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"dataset\": \"ktgiahieu/maccrobat2018_2020\",\n",
    "        \"final_f1\": eval_results[\"eval_f1\"],\n",
    "        \"final_precision\": eval_results[\"eval_precision\"],\n",
    "        \"final_recall\": eval_results[\"eval_recall\"],\n",
    "        \"num_epochs\": training_args.num_train_epochs,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add model files to artifact\n",
    "artifact.add_dir(local_model_path)\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "print(\" Model uploaded to W&B!\")\n",
    "\n",
    "# Log final summary statistics to W&B\n",
    "wandb.summary.update({\n",
    "    # Overall metrics\n",
    "    \"final_f1\": eval_results[\"eval_f1\"],\n",
    "    \"final_precision\": eval_results[\"eval_precision\"],\n",
    "    \"final_recall\": eval_results[\"eval_recall\"],\n",
    "    \"final_loss\": eval_results[\"eval_loss\"],\n",
    "    \n",
    "    # Training info\n",
    "    \"total_epochs\": training_args.num_train_epochs,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"num_labels\": len(id2tag),\n",
    "    \n",
    "    # Dataset info\n",
    "    \"train_samples\": len(tokenized_dataset[\"train\"]),\n",
    "    \"val_samples\": len(tokenized_dataset[\"validation\"]),\n",
    "    \n",
    "    # Best entity performance\n",
    "    \"best_entity_f1\": max(entity_f1_scores.values()) if entity_f1_scores else 0.0,\n",
    "    \"worst_entity_f1\": min(entity_f1_scores.values()) if entity_f1_scores else 0.0,\n",
    "})\n",
    "\n",
    "print(\"\\n All metrics logged to W&B\")\n",
    "print(f\" View run at: {wandb.run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add W&B logging for per-entity metrics\n",
    "from seqeval.metrics import classification_report\n",
    "import re\n",
    "\n",
    "# Get detailed per-entity report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions on validation set\n",
    "predictions, labels, _ = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "# Get classification report as string and print\n",
    "report_str = classification_report(true_labels, true_predictions)\n",
    "print(report_str)\n",
    "\n",
    "# Parse classification report and log to W&B\n",
    "# Extract per-entity metrics from classification report\n",
    "report_dict = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "\n",
    "# Log per-entity F1 scores to W&B\n",
    "entity_f1_scores = {}\n",
    "for entity_type, metrics in report_dict.items():\n",
    "    if entity_type not in ['micro avg', 'macro avg', 'weighted avg'] and isinstance(metrics, dict):\n",
    "        entity_f1_scores[f\"entity_f1/{entity_type}\"] = metrics.get('f1-score', 0.0)\n",
    "\n",
    "wandb.log(entity_f1_scores)\n",
    "\n",
    "#  Find which entity types are failing\n",
    "from collections import Counter\n",
    "pred_counts = Counter([tag for seq in true_predictions for tag in seq if tag != 'O'])\n",
    "gold_counts = Counter([tag for seq in true_labels for tag in seq if tag != 'O'])\n",
    "\n",
    "print(\"\\nEntity Distribution (Gold vs Predicted):\")\n",
    "print(f\"{'Entity Type':<30} {'Gold Count':<15} {'Pred Count':<15} {'Difference'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Prepare entity distribution data for W&B\n",
    "entity_distribution = {}\n",
    "for entity in sorted(gold_counts.keys()):\n",
    "    gold = gold_counts[entity]\n",
    "    pred = pred_counts.get(entity, 0)\n",
    "    diff = pred - gold\n",
    "    print(f\"{entity:<30} {gold:<15} {pred:<15} {diff:+d}\")\n",
    "    \n",
    "    #  Log to W&B\n",
    "    entity_distribution[f\"entity_dist/{entity}/gold\"] = gold\n",
    "    entity_distribution[f\"entity_dist/{entity}/predicted\"] = pred\n",
    "    entity_distribution[f\"entity_dist/{entity}/difference\"] = diff\n",
    "\n",
    "wandb.log(entity_distribution)\n",
    "\n",
    "# Create W&B Table for entity performance\n",
    "import pandas as pd\n",
    "\n",
    "entity_performance_data = []\n",
    "for entity_type, metrics in report_dict.items():\n",
    "    if entity_type not in ['micro avg', 'macro avg', 'weighted avg'] and isinstance(metrics, dict):\n",
    "        entity_performance_data.append({\n",
    "            'Entity Type': entity_type,\n",
    "            'Precision': metrics.get('precision', 0.0),\n",
    "            'Recall': metrics.get('recall', 0.0),\n",
    "            'F1-Score': metrics.get('f1-score', 0.0),\n",
    "            'Support': metrics.get('support', 0),\n",
    "            'Gold Count': gold_counts.get(entity_type, 0),\n",
    "            'Pred Count': pred_counts.get(entity_type, 0),\n",
    "        })\n",
    "\n",
    "entity_perf_df = pd.DataFrame(entity_performance_data)\n",
    "entity_perf_df = entity_perf_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "# Log as W&B Table\n",
    "wandb.log({\"entity_performance_table\": wandb.Table(dataframe=entity_perf_df)})\n",
    "\n",
    "print(\"\\n Per-entity metrics logged to W&B\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instructions for downloading model from W&B\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARTIFACT INFO\")\n",
    "print(\"=\"*80)\n",
    "print(f\" Model saved to W&B as artifact: 'pubmedbert-maccrobat-ner'\")\n",
    "print(f\" Artifact version: v{wandb.run.summary.get('_wandb', {}).get('artifact_version', 0)}\")\n",
    "print(f\"\\n To download and use this model later:\")\n",
    "print(f\"\"\"\n",
    "# Download from W&B\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('YOUR_USERNAME/biomedical-ner/pubmedbert-maccrobat-ner:latest')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load model\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(artifact_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Reload best model\n",
    "best_model = AutoModelForTokenClassification.from_pretrained(\"./pubmedbert-maccrobat-final\")\n",
    "nlp = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=best_model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Test on a sample\n",
    "test_text = \"A 67-year-old female presented with acute myocardial infarction and received aspirin.\"\n",
    "results = nlp(test_text)\n",
    "\n",
    "print(\"Test Inference:\")\n",
    "print(f\"Input: {test_text}\\n\")\n",
    "for result in results:\n",
    "    print(f\"  {result['word']:30s}  {result['entity_group']:30s} (conf: {result['score']:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

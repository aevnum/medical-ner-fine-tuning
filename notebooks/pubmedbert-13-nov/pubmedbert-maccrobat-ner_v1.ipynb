{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb seqeval --quiet\n!pip install protobuf==3.20.*","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\n# Load your secret\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# Login\nwandb.login(key=wandb_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:45:34.081756Z","iopub.execute_input":"2025-11-13T17:45:34.082009Z","iopub.status.idle":"2025-11-13T17:45:43.575315Z","shell.execute_reply.started":"2025-11-13T17:45:34.081989Z","shell.execute_reply":"2025-11-13T17:45:43.574767Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkshitij-nevrekar\u001b[0m (\u001b[33mkshitij-nevrekar-nmims\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForTokenClassification,\n    Trainer, \n    TrainingArguments\n)\nfrom datasets import load_dataset\nfrom seqeval.metrics import classification_report, f1_score\nimport wandb\n\n# Initialize W&B\nwandb.init(\n    project=\"biomedical-ner\",\n    name=\"pubmedbert-maccrobat-v1\",\n    tags=[\"pubmedbert\", \"maccrobat\", \"baseline\"]\n)\n\nprint(\"✅ W&B initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:45:47.852495Z","iopub.execute_input":"2025-11-13T17:45:47.852859Z","iopub.status.idle":"2025-11-13T17:46:30.168427Z","shell.execute_reply.started":"2025-11-13T17:45:47.852840Z","shell.execute_reply":"2025-11-13T17:46:30.167760Z"}},"outputs":[{"name":"stderr","text":"2025-11-13 17:46:02.152213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763055962.390832     107 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763055962.451725     107 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251113_174623-34fkj3ss</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/34fkj3ss' target=\"_blank\">pubmedbert-maccrobat-v1</a></strong> to <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner' target=\"_blank\">https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/34fkj3ss' target=\"_blank\">https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/34fkj3ss</a>"},"metadata":{}},{"name":"stdout","text":"✅ W&B initialized\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load MACCROBAT from HuggingFace\ndataset = load_dataset(\"ktgiahieu/maccrobat2018_2020\")\n\n# Check structure\nprint(f\"Train examples: {len(dataset['train'])}\")\nprint(f\"Example structure:\")\nexample = dataset['train'][0]\nprint(f\"  Keys: {example.keys()}\")\nprint(f\"  Tokens: {example['tokens'][:10]}\")\nprint(f\"  Tags: {example['tags'][:10]}\")\n\n# Get unique tags for num_labels\nunique_tags = set()\nfor example in dataset['train']:\n    unique_tags.update(example['tags'])\n\n# Ensure every B-XXX tag has a corresponding I-XXX tag\nextra_i_tags = set()\nfor tag in unique_tags:\n    if tag.startswith(\"B-\"):\n        i_tag = \"I-\" + tag[2:]\n        if i_tag not in unique_tags:\n            extra_i_tags.add(i_tag)\nunique_tags.update(extra_i_tags)\n\nunique_tags = sorted(list(unique_tags))\nnum_labels = len(unique_tags)\ntag2id = {tag: i for i, tag in enumerate(unique_tags)}\nid2tag = {i: tag for tag, i in tag2id.items()}\n\nprint(f\"\\n✅ Found {num_labels} unique entity types\")\nprint(f\"Entity types: {unique_tags[:10]}...\")  # Show first 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:46:34.864064Z","iopub.execute_input":"2025-11-13T17:46:34.864792Z","iopub.status.idle":"2025-11-13T17:46:36.814837Z","shell.execute_reply.started":"2025-11-13T17:46:34.864770Z","shell.execute_reply":"2025-11-13T17:46:36.814073Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11776747aa574870bf5755eb990039a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"325a9476a5de4b299ae8151316426d0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890f9fb5c48043a19f227696293091f1"}},"metadata":{}},{"name":"stdout","text":"Train examples: 400\nExample structure:\n  Keys: dict_keys(['tokens', 'tags'])\n  Tokens: ['A', '68', '-', 'year', '-', 'old', 'female', 'nonsmoker', ',', 'nondrinker']\n  Tags: ['O', 'B-Age', 'I-Age', 'I-Age', 'I-Age', 'I-Age', 'B-Sex', 'B-History', 'O', 'B-History']\n\n✅ Found 83 unique entity types\nEntity types: ['B-Activity', 'B-Administration', 'B-Age', 'B-Area', 'B-Biological_attribute', 'B-Biological_structure', 'B-Clinical_event', 'B-Color', 'B-Coreference', 'B-Date']...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load tokenizer\nmodel_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_and_align_labels(examples):\n    \"\"\"\n    Tokenize text and align labels with tokens.\n    \n    Key insight: BERT tokenizes words into subwords (e.g., \"myocardial\" → [\"my\", \"##ocardial\"]).\n    We need to align BIO tags with these subword tokens.\n    \n    Strategy: \n    - First subword of a word gets the original tag (B-DISEASE)\n    - Continuation subwords get I-tag version (I-DISEASE)\n    - Special tokens ([CLS], [SEP]) get ignored (label_id = -100)\n    \"\"\"\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,  # Already pre-tokenized\n        max_length=512,\n        padding=False  # We'll handle padding in data collator\n    )\n    \n    labels = []\n    for i, label in enumerate(examples[\"tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = []\n        previous_word_idx = None\n        \n        for word_idx in word_ids:\n            if word_idx is None:\n                # Special tokens → ignore in loss\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                # First subword of a word → use original label\n                label_ids.append(tag2id[label[word_idx]])\n            else:\n                # Continuation subwords → convert B-tag to I-tag\n                original_tag = label[word_idx]\n                if original_tag.startswith(\"B-\"):\n                    # B-DISEASE → I-DISEASE\n                    label_ids.append(tag2id[\"I-\" + original_tag[2:]])\n                else:\n                    # Keep I-tags as-is\n                    label_ids.append(tag2id[original_tag])\n            \n            previous_word_idx = word_idx\n        \n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Apply tokenization to dataset\nprint(\"Tokenizing dataset...\")\ntokenized_dataset = dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\nprint(f\"✅ Tokenized train examples: {len(tokenized_dataset['train'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:46:42.084184Z","iopub.execute_input":"2025-11-13T17:46:42.084902Z","iopub.status.idle":"2025-11-13T17:46:43.860070Z","shell.execute_reply.started":"2025-11-13T17:46:42.084876Z","shell.execute_reply":"2025-11-13T17:46:43.859488Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120cbe1272ba469d93079157dc064256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bac3f07f39548728b98bfce26f55197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c47df368fc641a5b965f98054c89be9"}},"metadata":{}},{"name":"stdout","text":"Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3fde43fe2b43a190c398c086fbace9"}},"metadata":{}},{"name":"stdout","text":"✅ Tokenized train examples: 400\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\n# Data collator pads sequences to same length in each batch\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\nprint(\"✅ Data collator created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:46:49.130776Z","iopub.execute_input":"2025-11-13T17:46:49.131500Z","iopub.status.idle":"2025-11-13T17:46:49.137578Z","shell.execute_reply.started":"2025-11-13T17:46:49.131474Z","shell.execute_reply":"2025-11-13T17:46:49.136907Z"}},"outputs":[{"name":"stdout","text":"✅ Data collator created\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def compute_metrics(p):\n    \"\"\"\n    Compute standard NER metrics: precision, recall, F1.\n    Uses seqeval for entity-level evaluation.\n    \"\"\"\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    \n    # Remove -100 labels (special tokens we ignored)\n    true_predictions = [\n        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    # Calculate metrics\n    results = {\n        \"precision\": f1_score(true_labels, true_predictions, average=\"macro\"),\n        \"recall\": f1_score(true_labels, true_predictions, average=\"macro\"),\n        \"f1\": f1_score(true_labels, true_predictions, average=\"micro\")\n    }\n    \n    return results\n\nprint(\"✅ Metrics function created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:55:23.785868Z","iopub.execute_input":"2025-11-13T17:55:23.786617Z","iopub.status.idle":"2025-11-13T17:55:23.794106Z","shell.execute_reply.started":"2025-11-13T17:55:23.786596Z","shell.execute_reply":"2025-11-13T17:55:23.793548Z"}},"outputs":[{"name":"stdout","text":"✅ Metrics function created\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Load PubMedBERT with token classification head\nmodel_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels\n)\n\nprint(f\"✅ Model loaded: {model_name}\")\nprint(f\"   Parameters: {model.num_parameters():,}\")\nprint(f\"   Entity types: {num_labels}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:46:58.910989Z","iopub.execute_input":"2025-11-13T17:46:58.911519Z","iopub.status.idle":"2025-11-13T17:47:02.610266Z","shell.execute_reply.started":"2025-11-13T17:46:58.911497Z","shell.execute_reply":"2025-11-13T17:47:02.609543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb03f932ab8409c98205d669bdf6f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf946d60731e4700807e4bdb13b28652"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"✅ Model loaded: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n   Parameters: 108,955,475\n   Entity types: 83\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./pubmedbert-maccrobat\",\n\n    # Data\n    per_device_train_batch_size=16,  # Kaggle T4 can handle this\n    per_device_eval_batch_size=32,\n\n    # Epochs & steps\n    num_train_epochs=3,  # Start with 3, increase if needed\n    eval_strategy=\"epoch\",  # Evaluate after each epoch\n\n    # Learning rate\n    learning_rate=2e-5,  # Standard for fine-tuning BERT\n    warmup_steps=500,   # Linear warmup for first 500 steps\n    weight_decay=0.01,\n\n    # Optimization\n    optim=\"adamw_torch\",\n    gradient_accumulation_steps=1,\n    max_grad_norm=1.0,\n\n    # Logging & saving\n    logging_steps=100,  # Log metrics every 100 steps\n    save_strategy=\"epoch\",  # Save model after each epoch\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n\n    # W&B integration\n    report_to=[\"wandb\"],\n    run_name=\"pubmedbert-maccrobat-v1\",\n\n    # Misc\n    seed=42,\n    fp16=True,  # Use mixed precision to save memory on T4\n)\n\nprint(\"✅ Training arguments configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:55:27.338678Z","iopub.execute_input":"2025-11-13T17:55:27.339249Z","iopub.status.idle":"2025-11-13T17:55:27.374720Z","shell.execute_reply.started":"2025-11-13T17:55:27.339228Z","shell.execute_reply":"2025-11-13T17:55:27.373938Z"}},"outputs":[{"name":"stdout","text":"✅ Training arguments configured\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# Split train set into train/validation (e.g., 90% train, 10% val)\nsplit_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\ntokenized_dataset = DatasetDict({\n    \"train\": split_dataset[\"train\"],\n    \"validation\": split_dataset[\"test\"]\n})\n\nprint(f\"Train size: {len(tokenized_dataset['train'])}\")\nprint(f\"Validation size: {len(tokenized_dataset['validation'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:53:19.427938Z","iopub.execute_input":"2025-11-13T17:53:19.428648Z","iopub.status.idle":"2025-11-13T17:53:19.443126Z","shell.execute_reply.started":"2025-11-13T17:53:19.428616Z","shell.execute_reply":"2025-11-13T17:53:19.442478Z"}},"outputs":[{"name":"stdout","text":"Train size: 360\nValidation size: 40\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\n\nprint(\"✅ Trainer initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:55:31.516785Z","iopub.execute_input":"2025-11-13T17:55:31.517478Z","iopub.status.idle":"2025-11-13T17:55:31.533102Z","shell.execute_reply.started":"2025-11-13T17:55:31.517455Z","shell.execute_reply":"2025-11-13T17:55:31.532367Z"}},"outputs":[{"name":"stdout","text":"✅ Trainer initialized\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_107/3751207074.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Train!\nprint(\"Starting training...\")\nprint(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n\ntrain_result = trainer.train()\n\nprint(\"\\n✅ Training complete!\")\nprint(f\"Final loss: {train_result.training_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:55:33.527676Z","iopub.execute_input":"2025-11-13T17:55:33.528217Z","iopub.status.idle":"2025-11-13T17:56:38.879088Z","shell.execute_reply.started":"2025-11-13T17:55:33.528192Z","shell.execute_reply":"2025-11-13T17:56:38.878353Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nDevice: Tesla T4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 01:02, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>4.491906</td>\n      <td>0.001358</td>\n      <td>0.001358</td>\n      <td>0.004616</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>4.427600</td>\n      <td>0.001392</td>\n      <td>0.001392</td>\n      <td>0.004710</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>4.316536</td>\n      <td>0.001468</td>\n      <td>0.001468</td>\n      <td>0.004898</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Training complete!\nFinal loss: 4.4471\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Evaluate on training set (TODO: use actual validation set)\nprint(\"Evaluating...\")\neval_results = trainer.evaluate()\n\nprint(\"\\nEvaluation Results:\")\nfor key, value in eval_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\n# Save final model\ntrainer.save_model(\"./pubmedbert-maccrobat-final\")\nprint(\"\\n✅ Model saved to ./pubmedbert-maccrobat-final\")\n\n# Log final metrics to W&B\nwandb.log(eval_results)\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:56:46.016171Z","iopub.execute_input":"2025-11-13T17:56:46.016714Z","iopub.status.idle":"2025-11-13T17:56:48.792394Z","shell.execute_reply.started":"2025-11-13T17:56:46.016691Z","shell.execute_reply":"2025-11-13T17:56:48.791614Z"}},"outputs":[{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nEvaluation Results:\n  eval_loss: 4.3165\n  eval_precision: 0.0015\n  eval_recall: 0.0015\n  eval_f1: 0.0049\n  eval_runtime: 0.8635\n  eval_samples_per_second: 46.3240\n  eval_steps_per_second: 1.1580\n  epoch: 3.0000\n\n✅ Model saved to ./pubmedbert-maccrobat-final\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁▃██</td></tr><tr><td>eval/loss</td><td>█▅▁▁</td></tr><tr><td>eval/precision</td><td>▁▃██</td></tr><tr><td>eval/recall</td><td>▁▃██</td></tr><tr><td>eval/runtime</td><td>▁▂▅█</td></tr><tr><td>eval/samples_per_second</td><td>█▇▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▄▁</td></tr><tr><td>eval_f1</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_precision</td><td>▁</td></tr><tr><td>eval_recall</td><td>▁</td></tr><tr><td>eval_runtime</td><td>▁</td></tr><tr><td>eval_samples_per_second</td><td>▁</td></tr><tr><td>eval_steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▅███</td></tr><tr><td>train/global_step</td><td>▁▅████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>eval/f1</td><td>0.0049</td></tr><tr><td>eval/loss</td><td>4.31654</td></tr><tr><td>eval/precision</td><td>0.00147</td></tr><tr><td>eval/recall</td><td>0.00147</td></tr><tr><td>eval/runtime</td><td>0.8635</td></tr><tr><td>eval/samples_per_second</td><td>46.324</td></tr><tr><td>eval/steps_per_second</td><td>1.158</td></tr><tr><td>eval_f1</td><td>0.0049</td></tr><tr><td>eval_loss</td><td>4.31654</td></tr><tr><td>eval_precision</td><td>0.00147</td></tr><tr><td>eval_recall</td><td>0.00147</td></tr><tr><td>eval_runtime</td><td>0.8635</td></tr><tr><td>eval_samples_per_second</td><td>46.324</td></tr><tr><td>eval_steps_per_second</td><td>1.158</td></tr><tr><td>total_flos</td><td>282407157227520.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>36</td></tr><tr><td>train_loss</td><td>4.44715</td></tr><tr><td>train_runtime</td><td>64.5488</td></tr><tr><td>train_samples_per_second</td><td>16.732</td></tr><tr><td>train_steps_per_second</td><td>0.558</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pubmedbert-maccrobat-v1</strong> at: <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/34fkj3ss' target=\"_blank\">https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner/runs/34fkj3ss</a><br> View project at: <a href='https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner' target=\"_blank\">https://wandb.ai/kshitij-nevrekar-nmims/biomedical-ner</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251113_174623-34fkj3ss/logs</code>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Load the trained model\nfrom transformers import pipeline\n\n# Reload best model\nbest_model = AutoModelForTokenClassification.from_pretrained(\"./pubmedbert-maccrobat-final\")\nnlp = pipeline(\n    \"token-classification\",\n    model=best_model,\n    tokenizer=tokenizer,\n    aggregation_strategy=\"simple\"\n)\n\n# Test on a sample\ntest_text = \"A 67-year-old female presented with acute myocardial infarction and received aspirin.\"\nresults = nlp(test_text)\n\nprint(\"Test Inference:\")\nprint(f\"Input: {test_text}\\n\")\nfor result in results:\n    print(f\"  {result['word']:30s} → {result['entity_group']:30s} (conf: {result['score']:.3f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:57:50.626383Z","iopub.execute_input":"2025-11-13T17:57:50.626968Z","iopub.status.idle":"2025-11-13T17:57:50.901211Z","shell.execute_reply.started":"2025-11-13T17:57:50.626948Z","shell.execute_reply":"2025-11-13T17:57:50.900610Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Test Inference:\nInput: A 67-year-old female presented with acute myocardial infarction and received aspirin.\n\n  a                              → LABEL_60                       (conf: 0.020)\n  67                             → LABEL_66                       (conf: 0.022)\n  - year - old                   → LABEL_60                       (conf: 0.023)\n  female                         → LABEL_56                       (conf: 0.020)\n  presented                      → LABEL_60                       (conf: 0.020)\n  with                           → LABEL_56                       (conf: 0.021)\n  acute                          → LABEL_36                       (conf: 0.021)\n  myocardial infarction          → LABEL_56                       (conf: 0.022)\n  and                            → LABEL_52                       (conf: 0.019)\n  received                       → LABEL_60                       (conf: 0.020)\n  aspirin                        → LABEL_52                       (conf: 0.020)\n  .                              → LABEL_56                       (conf: 0.020)\n","output_type":"stream"}],"execution_count":29}]}